{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a positional inverted index for phrase queries\n",
    "===========================================================\n",
    "\n",
    "In this week's exercise we will look at enhanced inverted indices.\n",
    "The second form we look at is the inverted index (cf. section 2.4.2 of the book).\n",
    "\n",
    "In this notebook we provide you with a mostly complete implementation for\n",
    "the boolean queries.\n",
    "\n",
    "Your task is to complete `add_document()` to build a positional index and\n",
    "to complete `positional_intersect_two()` to be able to process phrase\n",
    "queries on the positional index.\n",
    "\n",
    "A phrase query is a query such as \"Romans countrymen\" which should only return\n",
    "documents that contain this exact phrase.\n",
    "We updated the provided parser to support phrases in arbitrary boolean\n",
    "queries.\n",
    "\n",
    "Any phrase needs to be enclosed in double quotes for the query parser to be\n",
    "able to detect it.\n",
    "The output format for a phrase is a list of the individual words that make up\n",
    "the phrase, in order.\n",
    "\n",
    "Below we show a few example queries containing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AND: args=['wit', NOT: args=[['else', 'a', 'wit']], 'sonnet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from queryparser import parse_query, process_ast\n",
    "process_ast(parse_query('\"a wit by folly vanquished\"'))\n",
    "process_ast(parse_query('wit AND NOT \"else a wit\" AND sonnet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the imports and global variables in a separate cell, so we can rerun\n",
    "the code cell without loosing the contents of the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryparser import parse_query, ParseException, process_ast, Operation\n",
    "import glob\n",
    "import itertools\n",
    "from textutils import tokenize_document\n",
    "# global variables defining the index\n",
    "documents = dict()\n",
    "the_index = dict()\n",
    "docid_counter = 1\n",
    "# the path to the corpus\n",
    "corpuspath=\"../../shared/corpus/*.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to remove stop words from your flattened queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textutils import stop_words\n",
    "def remove_stop_words(ast):\n",
    "    new_args = []\n",
    "    for a in ast.args:\n",
    "        if isinstance(a, list):\n",
    "            new_args.append([x for x in a if not x in stop_words])\n",
    "        elif isinstance(a, Operation):\n",
    "            new_args.append(remove_stop_words(a))\n",
    "        elif a[0] == '-' and a[1:] in stop_words:\n",
    "            pass\n",
    "        elif a in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            new_args.append(a)\n",
    "    ast.args = new_args\n",
    "    return ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of `add_document`: the structure of `the_index` should be a `map` from words to lists of `(document_id, [list of positions])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document(doc):\n",
    "    '''\n",
    "    Add a document to the inverted index. Returns the document's ID\n",
    "    '''\n",
    "    global documents, docid_counter, the_index\n",
    "\n",
    "    # do not re-add the same document.\n",
    "    if doc in documents.values():\n",
    "        return documents[doc]\n",
    "    docid = docid_counter\n",
    "    documents[docid] = doc\n",
    "    docid_counter += 1\n",
    "    print(\"Adding document %s to inverted index with document ID %d\" % (doc, docid))\n",
    "    for pos, word in enumerate(tokenize_document(doc)):\n",
    "        docs = the_index.setdefault(word, [])\n",
    "        if len(docs) > 0 and docs[-1][0] == docid:\n",
    "            docs[-1][1].append(pos)\n",
    "        else:\n",
    "            docs.append((docid, [pos]))\n",
    "    return docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding document ../../shared/corpus\\alls_well_that_ends_well.txt to inverted index with document ID 1\n",
      "Adding document ../../shared/corpus\\as_you_like_it.txt to inverted index with document ID 2\n",
      "Adding document ../../shared/corpus\\a_lovers_complaint.txt to inverted index with document ID 3\n",
      "Adding document ../../shared/corpus\\a_midsummer_nights_dream.txt to inverted index with document ID 4\n",
      "Adding document ../../shared/corpus\\cymbeline.txt to inverted index with document ID 5\n",
      "Adding document ../../shared/corpus\\king_henry_the_eighth.txt to inverted index with document ID 6\n",
      "Adding document ../../shared/corpus\\king_john.txt to inverted index with document ID 7\n",
      "Adding document ../../shared/corpus\\king_richard_the_second.txt to inverted index with document ID 8\n",
      "Adding document ../../shared/corpus\\king_richard_the_third.txt to inverted index with document ID 9\n",
      "Adding document ../../shared/corpus\\loves_labours_lost.txt to inverted index with document ID 10\n",
      "Adding document ../../shared/corpus\\measure_for_measure.txt to inverted index with document ID 11\n",
      "Adding document ../../shared/corpus\\much_ado_about_nothing.txt to inverted index with document ID 12\n",
      "Adding document ../../shared/corpus\\pericles_prince_of_tyre.txt to inverted index with document ID 13\n",
      "Adding document ../../shared/corpus\\the_comedy_of_errors.txt to inverted index with document ID 14\n",
      "Adding document ../../shared/corpus\\the_first_part_of_henry_the_sixth.txt to inverted index with document ID 15\n",
      "Adding document ../../shared/corpus\\the_first_part_of_king_henry_the_fourth.txt to inverted index with document ID 16\n",
      "Adding document ../../shared/corpus\\the_history_of_troilus_and_cressida.txt to inverted index with document ID 17\n",
      "Adding document ../../shared/corpus\\the_life_of_king_henry_the_fifth.txt to inverted index with document ID 18\n",
      "Adding document ../../shared/corpus\\the_life_of_timon_of_athens.txt to inverted index with document ID 19\n",
      "Adding document ../../shared/corpus\\the_merchant_of_venice.txt to inverted index with document ID 20\n",
      "Adding document ../../shared/corpus\\the_merry_wives_of_windsor.txt to inverted index with document ID 21\n",
      "Adding document ../../shared/corpus\\the_passionate_pilgrim.txt to inverted index with document ID 22\n",
      "Adding document ../../shared/corpus\\the_phoenix_and_the_turtle.txt to inverted index with document ID 23\n",
      "Adding document ../../shared/corpus\\the_rape_of_lucrece.txt to inverted index with document ID 24\n",
      "Adding document ../../shared/corpus\\the_second_part_of_king_henry_the_fourth.txt to inverted index with document ID 25\n",
      "Adding document ../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt to inverted index with document ID 26\n",
      "Adding document ../../shared/corpus\\the_sonnets.txt to inverted index with document ID 27\n",
      "Adding document ../../shared/corpus\\the_taming_of_the_shrew.txt to inverted index with document ID 28\n",
      "Adding document ../../shared/corpus\\the_tempest.txt to inverted index with document ID 29\n",
      "Adding document ../../shared/corpus\\the_third_part_of_king_henry_the_sixth.txt to inverted index with document ID 30\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt to inverted index with document ID 31\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_coriolanus.txt to inverted index with document ID 32\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt to inverted index with document ID 33\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_julius_caesar.txt to inverted index with document ID 34\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_king_lear.txt to inverted index with document ID 35\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_macbeth.txt to inverted index with document ID 36\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_othello_moor_of_venice.txt to inverted index with document ID 37\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_romeo_and_juliet.txt to inverted index with document ID 38\n",
      "Adding document ../../shared/corpus\\the_tragedy_of_titus_andronicus.txt to inverted index with document ID 39\n",
      "Adding document ../../shared/corpus\\the_two_gentlemen_of_verona.txt to inverted index with document ID 40\n",
      "Adding document ../../shared/corpus\\the_two_noble_kinsmen.txt to inverted index with document ID 41\n",
      "Adding document ../../shared/corpus\\the_winters_tale.txt to inverted index with document ID 42\n",
      "Adding document ../../shared/corpus\\twelfth_night_or_what_you_will.txt to inverted index with document ID 43\n",
      "Adding document ../../shared/corpus\\venus_and_adonis.txt to inverted index with document ID 44\n"
     ]
    }
   ],
   "source": [
    "for f in glob.glob(corpuspath):\n",
    "    add_document(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the intersection of two posting lists with positional information (refer to the book). Be careful of the structure of the returned value: some of the code provided expects a list of tuples `(document_id, position_of_p1, position_of_p2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect_two(p1, p2, k):\n",
    "    '''\n",
    "    Intersect two posting lists according to pseudo-code in Introduction\n",
    "    to Information Retrieval, Figure 2.12\n",
    "    k is the max distance between the two words\n",
    "    Returns a list of tuples (document_id, position_of_p1, position_of_p2)\n",
    "    '''\n",
    "    answer = []\n",
    "    while p1 != [] and p2 != []:\n",
    "        if p1[0][0] == p2[0][0]:\n",
    "            l = []\n",
    "            pos1 = list(p1[0][1])\n",
    "            pos2 = list(p2[0][1])\n",
    "            while pos1 != []:\n",
    "                while pos2 != []:\n",
    "                    if abs(pos1[0] - pos2[0]) <= k:\n",
    "                        l.append(pos2[0])\n",
    "                    elif pos2[0] > pos1[0]:\n",
    "                        break\n",
    "                    pos2 = pos2[1:]\n",
    "                while l != [] and abs(l[0] - pos1[0]) > k:\n",
    "                    l = l[1:]\n",
    "                for ps in l:\n",
    "                    answer.append((p1[0][0], pos1[0], ps))\n",
    "                pos1 = pos1[1:]\n",
    "            p1 = p1[1:]\n",
    "            p2 = p2[1:]\n",
    "        elif p1[0][0] < p2[0][0]:\n",
    "            p1 = p1[1:]\n",
    "        else:\n",
    "            p2 = p2[1:]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs the intersection for a phrase query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect(words):\n",
    "    '''\n",
    "    Positionally intersect posting lists for a list of words\n",
    "    '''\n",
    "    assert(len(words) >= 0)\n",
    "    postings = [the_index[t] for t in words]\n",
    "    result = None\n",
    "    while len(postings) >= 2:\n",
    "        result = [(docid, [x[1] for x in g]) for docid, g\n",
    "                in itertools.groupby(positional_intersect_two(postings[-2], postings[-1], 1), key=lambda x: x[0])]\n",
    "        postings[-2:] = [result,]\n",
    "    return [x[0] for x in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the following test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# changed the order compared to online version as windows sort your file lexicographically.\n",
    "assert(positional_intersect(['wit', 'folly', 'vanquished']) == [40])\n",
    "assert(positional_intersect(['send', 'some']) == [9, 14, 15, 26, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has the boolean query features you implemented last week (and some more). It will invoke the `positional_intersect` function for phrase terms in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_two(p1, p2):\n",
    "    '''\n",
    "    Intersect two posting lists according to pseudo-code in Introduction\n",
    "    to Information Retrieval, Figure 1.6\n",
    "    '''\n",
    "    answer = []\n",
    "    while p1 != [] and p2 != []:\n",
    "        if p1[0] == p2[0]:\n",
    "            answer.append(p1[0])\n",
    "            p1 = p1[1:]\n",
    "            p2 = p2[1:]\n",
    "        elif p1[0] < p2[0]:\n",
    "            p1 = p1[1:]\n",
    "        else:\n",
    "            p2 = p2[1:]\n",
    "    return answer\n",
    "\n",
    "def strip_positions(postings):\n",
    "    return (x[0] for x in postings)\n",
    "\n",
    "def negate(term):\n",
    "    if term in the_index.keys():\n",
    "        return sorted(set(documents.keys()) - set(strip_positions(the_index[term])))\n",
    "    else:\n",
    "        # What is the correct negation of a term not \n",
    "        return list(strip_positions(documents.keys()))\n",
    "\n",
    "def intersect(terms):\n",
    "    '''\n",
    "    Intersect posting lists for a list of terms\n",
    "    Take into consideration phrase queries\n",
    "    '''\n",
    "    def process_term(t):\n",
    "        if isinstance(t, list):\n",
    "            return positional_intersect(t)\n",
    "        elif isinstance(t, Operation) and t.op == 'NOT' and isinstance(t.args[0], list):\n",
    "            return sorted(set(documents.keys()) - set(positional_intersect(t.args[0])))\n",
    "        elif isinstance(t, str) and t.startswith('-'):\n",
    "            return negate(t[1:])\n",
    "        else:\n",
    "            assert(isinstance(t, str))\n",
    "            return list(strip_positions(the_index[t]))\n",
    "\n",
    "    postings = [process_term(t) for t in terms]\n",
    "    # calculate word frequencies and sort term,freq pairs in ascending\n",
    "    # order by frequency\n",
    "    freqs = sorted([ (p, len(p)) for p in postings ], key=lambda x: x[1] )\n",
    "    result = freqs[0][0]\n",
    "    del freqs[0]\n",
    "    while freqs != [] and freqs != []:\n",
    "        result = intersect_two(result, freqs[0][0])\n",
    "        del freqs[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a boolean query on the inverted index. We only support single\n",
    "    operator queries ATM.  This method returns a list of document ids\n",
    "    which satisfy the query in no particular order (i.e. the order in\n",
    "    which the documents were added most likely :)).\n",
    "    '''\n",
    "    # We use a generated parser to transform the query from a string to an\n",
    "    # AST.\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s'\\n\" % query, e)\n",
    "        return None\n",
    "\n",
    "    # We preprocess the AST to flatten commutative operations, such as\n",
    "    # sequences of ANDs. We also transform 'NOT <term>' arguments into\n",
    "    # '-<term>' to allow smarter processing of AND NOT and OR NOT.\n",
    "    flat = remove_stop_words(process_ast(ast))\n",
    "\n",
    "    args = []\n",
    "    # go through arguments and fall back on recursive evaluation if we\n",
    "    # could not completely flatten the query\n",
    "    for arg in flat.args:\n",
    "        if isinstance(arg, Operation) and arg.op != 'NOT':\n",
    "            print(\"Cannot handle query '%s', aborting...\" % query)\n",
    "            return None\n",
    "        elif isinstance(arg, list):\n",
    "            # Assume it's a phrase\n",
    "            for w in arg:\n",
    "                assert(isinstance(w, str))\n",
    "            if any(w not in the_index.keys() for w in arg):\n",
    "                print(\"NOTE: Dropping phrase '%s' because no document contains it\" % ' '.join(arg))\n",
    "            else:\n",
    "                args.append(arg)\n",
    "        elif isinstance(arg, str) and not arg.startswith('-') and arg not in the_index.keys():\n",
    "            # Drop terms that don't occur in the vocabulary\n",
    "            print(\"NOTE: Dropping term '%s' because no document contains it\" % arg)\n",
    "        else:\n",
    "            args.append(arg)\n",
    "\n",
    "    if flat.op == 'OR':\n",
    "        results = set()\n",
    "        for arg in args:\n",
    "            if isinstance(arg, list):\n",
    "                results = results | set(positional_intersect(arg))\n",
    "            elif isinstance(arg, Operation) and arg.op == 'NOT' and isinstance(arg.args[0], list):\n",
    "                print(\"Query '%s' not supported\", query)\n",
    "                return None\n",
    "            elif arg.startswith('-'):\n",
    "                results = results | set(negate(arg[1:]))\n",
    "            else:\n",
    "                results = results | set(strip_positions(the_index[arg]))\n",
    "        return sorted(results)\n",
    "\n",
    "    elif flat.op == 'AND':\n",
    "        return intersect(args)\n",
    "\n",
    "    elif flat.op == 'NOT':\n",
    "        # handle case where we query for the negation of a term not in the\n",
    "        # vocabulary.\n",
    "        if len(args) == 0:\n",
    "            return list(documents.keys())\n",
    "        else:\n",
    "            assert(len(args) == 1)\n",
    "            if isinstance(args[0], list):\n",
    "                return sorted(set(documents.keys()) - set(positional_intersect(args[0])))\n",
    "            else:\n",
    "                return sorted(set(documents.keys()) - set(strip_positions(the_index[args[0]])))\n",
    "\n",
    "    elif flat.op == 'LOOKUP':\n",
    "        if len(args) == 0:\n",
    "            # we drop terms that are not in the vocabulary in the\n",
    "            # preprocessing loop, so handle the case where we query for a\n",
    "            # single term that is not in the vocabulary.\n",
    "            return []\n",
    "        else:\n",
    "            # in this case the query was a single term\n",
    "            assert(len(args) == 1)\n",
    "            if isinstance(args[0], list):\n",
    "                return positional_intersect(args[0])\n",
    "            else:\n",
    "                return list(strip_positions(the_index[args[0]]))\n",
    "    else:\n",
    "        print(\"Cannot handle query '%s', aborting...\" % query)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(docs):\n",
    "    '''\n",
    "    Helper function to convert a list of document IDs back to file names\n",
    "    '''\n",
    "    if not docs:\n",
    "        print(\"No documents found\")\n",
    "        print()\n",
    "        return\n",
    "    # If we got some results, print them\n",
    "    for doc in docs:\n",
    "        print('%d -> %s' % (doc, documents[doc]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the following queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 -> ../../shared/corpus\\the_two_gentlemen_of_verona.txt\n",
      "\n",
      "1 -> ../../shared/corpus\\alls_well_that_ends_well.txt\n",
      "10 -> ../../shared/corpus\\loves_labours_lost.txt\n",
      "12 -> ../../shared/corpus\\much_ado_about_nothing.txt\n",
      "18 -> ../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "43 -> ../../shared/corpus\\twelfth_night_or_what_you_will.txt\n",
      "\n",
      "19 -> ../../shared/corpus\\the_life_of_timon_of_athens.txt\n",
      "32 -> ../../shared/corpus\\the_tragedy_of_coriolanus.txt\n",
      "38 -> ../../shared/corpus\\the_tragedy_of_romeo_and_juliet.txt\n",
      "39 -> ../../shared/corpus\\the_tragedy_of_titus_andronicus.txt\n",
      "43 -> ../../shared/corpus\\twelfth_night_or_what_you_will.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(execute_query('\"a wit by folly vanquished\"'))\n",
    "print_result(execute_query('wit AND NOT \"else a wit\" AND sonnet'))\n",
    "print_result(execute_query('\"glooming peace\" OR Titus'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
