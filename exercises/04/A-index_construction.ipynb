{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "starting-security",
   "metadata": {},
   "source": [
    "# Index construction\n",
    "\n",
    "This week's exercise is about constructing an inverted index in more detail.\n",
    "\n",
    "In particular, for the programming exercise, we will look at blocked\n",
    "sort-based indexing.  This is an indexing technique that can be employed when\n",
    "the full index, or some intermediate datastructures do not fit in the main\n",
    "memory of the computer performing the indexing.\n",
    "\n",
    "\n",
    "The basic algorithm is outlined in the book in Figure 4.2.  We will provide\n",
    "you with some code that produces a block and reads and writes blocks from and\n",
    "to disk, and leave the implementation of `bsbi_invert()` and the merging part\n",
    "of `merge_blocks()` to you.\n",
    "\n",
    "The format of the block you get from `parse_next_block()` is a potentially\n",
    "unordered list of `(termid, docid)` pairs.  The global datastructure\n",
    "`termid_map` can be used to look up term ids by term and the global\n",
    "datastructure `docid_map` can be used to look up documents by id.\n",
    "\n",
    "Our provided implementation of `parse_corpus()` and `parse_next_block()`\n",
    "produces the same document and token ids across executions.\n",
    "\n",
    "**Please note:** Here in the exercise we use an \"optimized\" version of BSBI. With BSBI as you have seen it in the lecture, the intermediate files are lists of raw termID-docID pairs, not postings lists. The postings list is only built at the very end when merging all the intermediate files. It is SPIMI that uses already built partial postings lists as intermediate representation. If this difference is not fully clear to you, we strongly advise you to revise the slides or the book.\n",
    "\n",
    "First we define some variables. Note the use of python's `namedtuple` factory function which is used here to create simple tuple subclasses that allow us to access tuple fields by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "from collections import namedtuple\n",
    "from textutils import tokenize_document\n",
    "\n",
    "Document = namedtuple('Document', ['path', 'id'])\n",
    "PostingsList = namedtuple('PostingsList', ['termid', 'postings'])\n",
    "\n",
    "docid_map = {}\n",
    "documentid_counter = 1\n",
    "termid_map = {}\n",
    "termid_counter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-valve",
   "metadata": {},
   "source": [
    "The following function parses the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def parse_corpus(corpus='solution-corpus.txt'):\n",
    "    '''\n",
    "    Parse corpus from input file and populate document-id map, return list of\n",
    "    documents as Document() tuples.\n",
    "    '''\n",
    "    global docid_map\n",
    "    documents = []\n",
    "    documentid_counter = 1\n",
    "    for path in sorted(glob.glob('../../shared/corpus/*.txt')):\n",
    "        documents.append(Document(path=path, id=documentid_counter))\n",
    "        docid_map[documentid_counter] = path\n",
    "        documentid_counter += 1\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = parse_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-request",
   "metadata": {},
   "source": [
    "Let's preview the first few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-sleeping",
   "metadata": {},
   "source": [
    "The following creates a directory for the block storage, the index and other auxiliary files. It deletes data from a previous run if it exists, so you can try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'output'\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-diabetes",
   "metadata": {},
   "source": [
    "#### Task 1:\n",
    "\n",
    "Implement `bsbi_invert()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bsbi_invert(block):\n",
    "    '''\n",
    "    Compute inverted index for block.\n",
    "    Return inverted index as (sorted by termid) list of PostingsList() tuples.\n",
    "    '''\n",
    "    postings = []\n",
    "    # Assignment: implement construction of inverted index for list of tuples\n",
    "    # given in block\n",
    "    # 1. Sort block contents by term id.\n",
    "    # Hint python's sort optionally takes a key function, see e.g.\n",
    "    # https://developers.google.com/edu/python/sorting#custom-sorting-with-key\n",
    "    # 2. Construct inverted index from termid-docid pairs\n",
    "    ### TODO: assignment\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-joshua",
   "metadata": {},
   "source": [
    "#### Task 2:\n",
    "Complete the implementation of `merge_blocks()`. For this, you first need to implement the helper function `select_next_readbufs()`. It gets called in order to return the termid of the term whose postings list is the next to be merged. In the next step, you then have to implement this merging. To do this, make use of the provided helper function `merge()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_termid(readbufs):\n",
    "    '''\n",
    "    Select next termid for which to compute complete postings list\n",
    "\n",
    "    readbufs is the list of input read buffers. This list may contain None for\n",
    "    input files which we have already processed completely.\n",
    "    '''\n",
    "    # Assignment: implement an algorithm to select the next termid for which\n",
    "    # we will merge postings lists\n",
    "    ### TODO: assignment\n",
    "    return None\n",
    "\n",
    "\n",
    "MERGE_READ_WINDOW = 500\n",
    "def refill_read_buffer(fh):\n",
    "    '''\n",
    "    Read a new buffer for read window in external merge from file at fh.\n",
    "    '''\n",
    "    b = []\n",
    "    for _ in range(MERGE_READ_WINDOW):\n",
    "        p = read_postings_list(fh)\n",
    "        if not p:\n",
    "            # Hit end-of-file, return what we've read so far.\n",
    "            break\n",
    "        b.append(p)\n",
    "    return b\n",
    "\n",
    "def merge(postings1, postings2):\n",
    "    '''\n",
    "    A helper function that merges the two postings lists postings1 and\n",
    "    postings2.\n",
    "    '''\n",
    "    return sorted(set(postings1) | set(postings2))\n",
    "\n",
    "\n",
    "def merge_blocks(blockfiles, outfile):\n",
    "    '''\n",
    "    Merge all the blocks stored in the files listed in blockfiles.\n",
    "    Write the merged index to outfile.\n",
    "    '''\n",
    "\n",
    "    # open all input files\n",
    "    inputs = []\n",
    "    readbufs = []\n",
    "    for f in blockfiles:\n",
    "        print(\"Opening file\", f, \"for input\")\n",
    "        inputs.append(open(os.path.join(OUTPUT_DIR, f)))\n",
    "    # open output file\n",
    "    outf = open(outfile, 'w')\n",
    "\n",
    "    # fill read buffers\n",
    "    for fh in inputs:\n",
    "        readbufs.append(refill_read_buffer(fh))\n",
    "\n",
    "    # Iterate over all the read buffers, create merged postings lists and\n",
    "    # write those to output file.\n",
    "\n",
    "    # While there is inputdata left\n",
    "    while sum(map(len, filter(None, readbufs))) > 0:\n",
    "        # Exercise: implement select_next_termid()\n",
    "        next_termid = select_next_termid(readbufs)\n",
    "        #print(\"Merging postings lists for termid=%d\" % next_termid)\n",
    "        postings = []\n",
    "\n",
    "        # Assignment: Go over all readbufs and merge postings lists for selected termid\n",
    "        for readbuf in readbufs:\n",
    "            # Hint: to make the surrounding code work, you should remove any items\n",
    "            # which you process from the read buffers.\n",
    "            # You can do this using the following statement: `del readbuf[0]`\n",
    "            # Make use of already implemented merge()\n",
    "            ### TODO: assignment\n",
    "            pass\n",
    "\n",
    "        # Write merged postings list to output file\n",
    "        write_postings_list(outf, PostingsList(termid=next_termid, postings=postings))\n",
    "\n",
    "        # If necessary, refill read buffers, this code assumes that the\n",
    "        # postings list merging removes processed elements from the read\n",
    "        # buffers using e.g. del readbuf[0].\n",
    "        # Additionally, this will replace the readbuf list for an input block\n",
    "        # with None, if we processed all the contents of that file.\n",
    "        for n,r in enumerate(readbufs):\n",
    "            if r is not None and len(r) == 0:\n",
    "                # print(\"Refilling read buffer for block\", n+1)\n",
    "                next = refill_read_buffer(inputs[n])\n",
    "                if len(next) == 0:\n",
    "                    print(\"> Finished processing input block\", n+1)\n",
    "                    readbufs[n] = None\n",
    "                else:\n",
    "                    readbufs[n] = next\n",
    "\n",
    "    # Close input files\n",
    "    for fh in inputs:\n",
    "        fh.close()\n",
    "    # Close output file\n",
    "    outf.flush()\n",
    "    outf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-criminal",
   "metadata": {},
   "source": [
    "In the following you find the function `blocked_sort_based_index` which calls your previous implemented functions and some other already provided helper functions. You don't have to change this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_gen = None\n",
    "def token_generator(documents):\n",
    "    '''\n",
    "    This method uses the global variable `token_gen` to create a single\n",
    "    instance of the generator iterator defined in _token_generator().\n",
    "    This allows us to iterate through all the tokens in all the documents\n",
    "    without having to carry a lot of information between calls to\n",
    "    `parse_next_block()` (see below).\n",
    "\n",
    "    The generator defined in _token_generator() returns (term, docid) tuples\n",
    "    for all documents but produces only one tuple for a term per document.\n",
    "\n",
    "    A generator with raise a StopIteration exception when there are no more\n",
    "    results to be produced.\n",
    "    '''\n",
    "    global token_gen\n",
    "    def _token_generator(documents):\n",
    "        for doc in documents:\n",
    "            doctokens = set()\n",
    "            for t in tokenize_document(doc.path):\n",
    "                if t in doctokens:\n",
    "                    continue\n",
    "                doctokens.add(t)\n",
    "                yield (t, doc.id)\n",
    "    if token_gen is None:\n",
    "        token_gen = _token_generator(documents)\n",
    "    return token_gen\n",
    "\n",
    "BLOCKSIZE = 10000\n",
    "def parse_next_block(documents):\n",
    "    '''\n",
    "    Produce termid-documentid tuples in batches of BLOCKSIZE.\n",
    "\n",
    "    This uses the token_generator() method to create blocks of (termid, docid)\n",
    "    tuples of fixed size.  See the docstring of token_generator() for more\n",
    "    details on how the token tuple generation works internally.  This method\n",
    "    also maps terms to term ids and stores that mapping in `termid_map`.\n",
    "\n",
    "    Note that the last block produced by this method may contain fewer than\n",
    "    BLOCKSIZE tuples.\n",
    "    '''\n",
    "    global termid_map, termid_counter\n",
    "    block = []\n",
    "    while len(block) < BLOCKSIZE:\n",
    "        try:\n",
    "            t, docid = next(token_generator(documents))\n",
    "        except StopIteration:\n",
    "            break\n",
    "        if t not in termid_map:\n",
    "            termid_map[t] = termid_counter\n",
    "            termid_counter += 1\n",
    "        tid = termid_map[t]\n",
    "        block.append((tid, docid))\n",
    "    return block\n",
    "\n",
    "def write_postings_list(fh, p):\n",
    "    '''\n",
    "    Write a PostingsList() to file at fh.\n",
    "    Format:  'termid:docid,docid,...,docid'\n",
    "    '''\n",
    "    # Custom format, for easier lazy reads\n",
    "    fh.write(\"%d:%s\\n\" % (p.termid, ','.join(map(str, p.postings))))\n",
    "\n",
    "def read_postings_list(fh):\n",
    "    '''\n",
    "    Read postings list from file at fh\n",
    "    Expected format:  'termid:docid,docid,...,docid'\n",
    "    '''\n",
    "    line = fh.readline().strip()\n",
    "    try:\n",
    "        termid, postingstr = line.split(':')\n",
    "        postings = list(map(int, postingstr.split(',')))\n",
    "        return PostingsList(termid=int(termid), postings=postings)\n",
    "    except:\n",
    "        if line != '':\n",
    "            # This is an actual parse error, empty line just signals EOF.\n",
    "            print(\"Unable to parse line '%s' as postings list\" % line)\n",
    "        return None\n",
    "\n",
    "\n",
    "def blocked_sort_based_index(documents, fullidx):\n",
    "    '''\n",
    "    Implement blocked sort based index as outlined in figure 4.2 of the book.\n",
    "    '''\n",
    "    n = 0\n",
    "    blockfiles = []\n",
    "    # We use this infinite loop to express a do-while loop\n",
    "    while True:\n",
    "        n += 1\n",
    "        block = parse_next_block(documents)\n",
    "        # We exit our do-while loop when we get a zero-length block from\n",
    "        # parse_next_block().\n",
    "        if len(block) == 0:\n",
    "            break\n",
    "        print(\"Indexing block\", n)\n",
    "        block = bsbi_invert(block)\n",
    "        fname = \"block%02d.txt\" % n\n",
    "        blockfiles.append(fname)\n",
    "        print(\"Storing block\", n, \"in\", fname)\n",
    "        with open(os.path.join(OUTPUT_DIR, fname), 'w') as fn:\n",
    "            for p in block:\n",
    "                write_postings_list(fn, p)\n",
    "\n",
    "    print(\"Merge blocks and store result in\", fullidx)\n",
    "    merge_blocks(blockfiles, fullidx)\n",
    "    print(\"Store termid and documentid maps\")\n",
    "    with open(os.path.join(OUTPUT_DIR, 'termid_map.txt'), 'w') as fh:\n",
    "        for term in termid_map.keys():\n",
    "            fh.write(\"%s:%d\\n\" % (term, termid_map[term]))\n",
    "    with open(os.path.join(OUTPUT_DIR, 'docid_map.txt'), 'w') as fh:\n",
    "        for docid in docid_map.keys():\n",
    "            fh.write(\"%d:%s\\n\" % (docid, docid_map[docid]))\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-curtis",
   "metadata": {},
   "source": [
    "Final function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_sort_based_index(documents, os.path.join(OUTPUT_DIR, 'index.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-toolbox",
   "metadata": {},
   "source": [
    "Now you should have a complete inverted index using term ids and document ids\n",
    "in `index.txt`.  You should open [index.txt](./output/index.txt) and\n",
    "[termid_map.txt](./output/termid_map.txt) and [docid_map.txt](./output/docid_map.txt) by\n",
    "clicking on the links or on the files in Jupyter to see how such an index\n",
    "might look stored on disk.\n",
    "\n",
    "A real index would probably use a more sophisticated file-format, such as a\n",
    "B-tree variant, which lends itself to looking up elements of the index more\n",
    "easily than our format which is just a textual representation of the inverted\n",
    "index, where we have one term id and its postings list on a line.\n",
    "\n",
    "A few sanity checks, assuming that you have not changed the way the handout\n",
    "code assigns document and term ids.  Note that from here on out, we will drop\n",
    "the pretense of our index not fitting into memory for simplicty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = {}\n",
    "with open(os.path.join(OUTPUT_DIR, 'index.txt')) as idx:\n",
    "    while True:\n",
    "        pl = read_postings_list(idx)\n",
    "        if not pl:\n",
    "            break\n",
    "        full_index[pl.termid] = pl.postings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-liver",
   "metadata": {},
   "source": [
    "First some quick checks to ensure that the term ids match our expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(termid_map['OF'] == 4005)\n",
    "assert(termid_map['eastern'] == 2977)\n",
    "assert(termid_map['heavnly'] == 10527)\n",
    "assert(termid_map['conflict'] == 15670)\n",
    "assert(termid_map['Brutus'] == 22919)\n",
    "assert(termid_map['Calpurnia'] == 32684)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(termid_map['OF'])\n",
    "print(termid_map['eastern'])\n",
    "print(termid_map['heavnly'])\n",
    "print(termid_map['conflict'])\n",
    "print(termid_map['Brutus'])\n",
    "print(termid_map['Calpurnia'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-shoot",
   "metadata": {},
   "source": [
    "Now we do some spot checks to see if the terms from above have the correct\n",
    "postings lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# term id 8: OF\n",
    "assert(full_index[4005] ==\n",
    "        [3, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40])\n",
    "# term id 5681: eastern\n",
    "assert(full_index[2977] == [2, 8, 17, 24, 38])\n",
    "# term id 10527: heavnly\n",
    "assert(full_index[10527] == [6,41])\n",
    "# term id 15670: conflict\n",
    "assert(full_index[15670] == [10,12,19,26,30,35,36,39,44])\n",
    "# term id 22919: Brutus\n",
    "assert(full_index[22919] == [18,20,24,26,31,32,33,34,39])\n",
    "# term id 32684: Calpurnia\n",
    "assert(full_index[32684] == [34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# term id 8: OF\n",
    "print(full_index[4005])\n",
    "# term id 5681: eastern\n",
    "print(full_index[2977])\n",
    "# term id 10527: heavnly\n",
    "print(full_index[10527])\n",
    "# term id 15670: conflict\n",
    "print(full_index[15670])\n",
    "# term id 22919: Brutus\n",
    "print(full_index[22919])\n",
    "# term id 32684: Calpurnia\n",
    "print(full_index[32684])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-festival",
   "metadata": {},
   "source": [
    "As a final test of the disk-based index we provide our implementation of the\n",
    "standard boolean query engine -- the one you had to implement in exercise 1 --\n",
    "adapted to construct its internal in-memory index from the on-disk index\n",
    "created by the blocked sort-based index construction algorithm.\n",
    "\n",
    "You should be able to query your disk-based index using the syntax below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryengine import InvertedIndex\n",
    "ii = InvertedIndex(indexfile=os.path.join(OUTPUT_DIR, 'index.txt'),\n",
    "                   termmap=os.path.join(OUTPUT_DIR, 'termid_map.txt'),\n",
    "                   docmap=os.path.join(OUTPUT_DIR, 'docid_map.txt'))\n",
    "\n",
    "# Expected: 34 -> ../../shared/corpus/the_tragedy_of_julius_caesar.txt\n",
    "ii.execute_and_print(\"Brutus AND Calpurnia\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
