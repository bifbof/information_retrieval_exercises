{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Model\n",
    "==================\n",
    "\n",
    "In this week's assignment you will implement the core parts of a ranked\n",
    "retrieval system which uses the vector space model to score documents for a\n",
    "given query.\n",
    "\n",
    "The overall structure of this assignment is similar to the Boolean Retrieval\n",
    "programming exercises in previous weeks.\n",
    "\n",
    "Note, however, that for this system, you do not (and indeed cannot without\n",
    "considerable effort) handle boolean queries, but just free form text queries.\n",
    "\n",
    "Because everything is free-form, and we still remove stop words when adding\n",
    "documents to the index, we give you a function `remove_stop_words` which will\n",
    "take a query string and return a list of query terms with the stop words\n",
    "removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textutils import tokenize_document, remove_stop_words\n",
    "import math, glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have global variables for the index, the document id to document mapping\n",
    "and a documentid counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Map document titles to document ids'''\n",
    "documents = {}\n",
    "'''A running counter for assigning numerical IDs to documents'''\n",
    "docid_counter = 1\n",
    "'''The document-term frequencies'''\n",
    "the_index = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to build the index. For the vector space model, we need to have\n",
    "different information in the index than just regular postings lists. We now\n",
    "need to store the frequency of each term for each document.\n",
    "\n",
    "The two quantities we need to know for each term in the collection of\n",
    "documents are (i) the number of documents in which the term appears, and (ii)\n",
    "the number of times a term appears for each of the documents that contain the\n",
    "term.\n",
    "\n",
    "We leave it up to you how exactly you want to store these quantitities, and\n",
    "ask you to implement getter functions that match your storage scheme for the\n",
    "two quantities below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document(doc):\n",
    "    '''\n",
    "    Add a document to the inverted index. Returns the document's ID\n",
    "    '''\n",
    "    global documents, docid_counter, the_index\n",
    "    # do not re-add the same document.\n",
    "    if doc in documents.values():\n",
    "        return\n",
    "    docid = docid_counter\n",
    "    documents[docid] = doc\n",
    "    docid_counter += 1\n",
    "    print(\"Adding document %s to inverted index with document ID %d\" % (doc, docid))\n",
    "    for term in tokenize_document(doc):\n",
    "        # TODO: collect term frequencies and document frequencies per term.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in glob.glob('../../shared/corpus/*.txt'):\n",
    "    add_document(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need wrappers to extract and compute the various numbers and formulas\n",
    "presented in the book in sections 6.2 and 6.3.\n",
    "\n",
    "First you have to implement the function `tf` that gives us the term frequency\n",
    "$tf_{t,d}$ of a term `term` in document with the id `docid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, docid):\n",
    "    '''\n",
    "    Calculate term frequency for term in docid. Return 0 if term not in index,\n",
    "    or term does not appear in document.\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you have to implement function `df` which gives us the document\n",
    "frequency $df_t$ of a term `term`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(term):\n",
    "    '''\n",
    "    Extract frequency of term for document with id docid from index\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to implement the two functions `tf` and `df` to extract term and\n",
    "per-term document frequencies from the index you've created in `add_document`.\n",
    "\n",
    "From here, you should use the functions `tf` and `df` where appropriate.\n",
    "\n",
    "Next you can implement the function `idf` to compute the inverse document\n",
    "frequency of a term t: $idf_t = log \\frac{N}{df_t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term):\n",
    "    '''\n",
    "    Compute idf_t for a term\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the parts that we need to compute the tf-idf weight for a term\n",
    "in a document.\n",
    "\n",
    "The tf-idf weighting scheme assigns weights with the following formula:\n",
    "$\\textit{tf-idf}_{t,d} = tf_{t,d} \\cdot idf_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(term, docid):\n",
    "    '''\n",
    "    Compute tf-idf for term and docid\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last helper function we will need is a normalization function. For this\n",
    "assignment we use cosine normalization, which is\n",
    "$C = \\sqrt{w_1^2 + w_2^2 + \\ldots + w_M^2}$ where\n",
    "$w_i = \\textit{tf-idf}_{t,docid} \\forall t \\in \\{\\text{terms contained in document with } docid\\}$.\n",
    "\n",
    "Note that in the book the cosine normalization is given as $\\frac{1}{C}$ in\n",
    "Figure 6.15, but if you already compute the reciprocal of $C$ in `norm_cosine`\n",
    "you would have to subsitute a multiplication for the division in line 9 of the\n",
    "algorithm in Figure 6.14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_cosine(docid):\n",
    "    '''\n",
    "    Compute cosine normalization for docid\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the cosine scoring function given as pseudo-code in\n",
    "Figure 6.14.  We give the variable `K` which determines how many results we\n",
    "return as an optional parameter.\n",
    "\n",
    "We give you the step \"Initialize Length[N]\", as (i) the book is not very clear on\n",
    "what you should do for that step and (ii) this array is strictly not\n",
    "necessary, as we could just call `norm_cosine` when we normalize the scores in\n",
    "step 2 of `cosine_score`.\n",
    "\n",
    "We leave it up to you to implement `cosine_score` using either the\n",
    "term-at-a-time or document-at-a-time strategies, as the choice of strategy\n",
    "should not change the results in any way.\n",
    "\n",
    "For the terms $wf_{t,d}$ and $w_{t,q}$ shown on line 6 of the algorithm in\n",
    "Figure 6.14 you should substitute the weighted frequency of term $t$ for\n",
    "document $d$ and the weight of term $t$ with respect to the query.\n",
    "\n",
    "You can pick a formula for each weight by picking a pair of expressions from\n",
    "the first two columns (Term frequency and Document frequency) of the table in\n",
    "Figure 6.15.\n",
    "\n",
    "The provided code uses cosine normalization for the document weights -- that's\n",
    "the provided code for step \"Initialize Length[N]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_score(query, K=10):\n",
    "    '''\n",
    "    Compute cosine scores for query `query` and return the top K results\n",
    "    according to Figure 6.14.\n",
    "    '''\n",
    "    # Initialize arrays so we can use docids as indices\n",
    "    scores = [0] * (len(documents.keys())+1)\n",
    "    length = [0] * (len(documents.keys())+1)\n",
    "    # Precompute length array values -- these are the normalization factors\n",
    "    for d in documents.keys():\n",
    "        length[d] = norm_cosine(d)\n",
    "    # TODO: implement\n",
    "    # 1. compute scores for each document, store in scores array\n",
    "    # 2. Normalize using the length array\n",
    "    # 3. Return top-K: sort by descending score and return first K elements of result.\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a scoring function, we can write `execute_query` as a thin\n",
    "wrapper around the scoring function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a string query on the vector space model index.\n",
    "    '''\n",
    "    # We use the new remove_stop_words helper function to split and normalize\n",
    "    # the query. After this call query is a list of words to query for\n",
    "    query = remove_stop_words(query)\n",
    "\n",
    "    return cosine_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we provide a `print_result` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(results):\n",
    "    if not results:\n",
    "        print(\"No documents found\")\n",
    "        print()\n",
    "        return\n",
    "    else:\n",
    "        # If we got some results, print them\n",
    "        for docid, score in results:\n",
    "            print('%2d(%6.3f) -> %s' % (docid, score, documents[docid]))\n",
    "        print()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run some queries on our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(execute_query(\"Brutus Calpurnia\"))\n",
    "print_result(execute_query(\"Caesar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the query \"Brutus Calpurnia\" the document\n",
    "`the_tragedy_of_julius_caesar.txt` should have by far the highest score.\n",
    "\n",
    "For the query \"Caesar\" the top three documents should be (in order):\n",
    "`the_tragedy_of_julius_caesar.txt` `the_tragedy_of_antony_and_cleopatra.txt`\n",
    "and `cymbeline.txt`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
