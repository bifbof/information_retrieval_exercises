{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a positional inverted index for phrase queries\n",
    "===========================================================\n",
    "\n",
    "In this week's exercise we will look at enhanced inverted indices.\n",
    "The second form we look at is the inverted index (cf. section 2.4.2 of the book).\n",
    "\n",
    "In this notebook we provide you with a mostly complete implementation for\n",
    "the boolean queries.\n",
    "\n",
    "Your task is to complete `add_document()` to build a positional index and\n",
    "to complete `positional_intersect_two()` to be able to process phrase\n",
    "queries on the positional index.\n",
    "\n",
    "A phrase query is a query such as \"Romans countrymen\" which should only return\n",
    "documents that contain this exact phrase.\n",
    "We updated the provided parser to support phrases in arbitrary boolean\n",
    "queries.\n",
    "\n",
    "Any phrase needs to be enclosed in double quotes for the query parser to be\n",
    "able to detect it.\n",
    "The output format for a phrase is a list of the individual words that make up\n",
    "the phrase, in order.\n",
    "\n",
    "Below we show a few example queries containing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryparser import parse_query, process_ast\n",
    "process_ast(parse_query('\"a wit by folly vanquished\"'))\n",
    "process_ast(parse_query('wit AND NOT \"else a wit\" AND sonnet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the imports and global variables in a separate cell, so we can rerun\n",
    "the code cell without loosing the contents of the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryparser import parse_query, ParseException, process_ast, Operation\n",
    "import glob\n",
    "import itertools\n",
    "from textutils import tokenize_document\n",
    "# global variables defining the index\n",
    "documents = dict()\n",
    "the_index = dict()\n",
    "docid_counter = 1\n",
    "# the path to the corpus\n",
    "corpuspath=\"../../shared/corpus/*.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to remove stop words from your flattened queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textutils import stop_words\n",
    "def remove_stop_words(ast):\n",
    "    new_args = []\n",
    "    for a in ast.args:\n",
    "        if isinstance(a, list):\n",
    "            new_args.append([x for x in a if not x in stop_words])\n",
    "        elif isinstance(a, Operation):\n",
    "            new_args.append(remove_stop_words(a))\n",
    "        elif a[0] == '-' and a[1:] in stop_words:\n",
    "            pass\n",
    "        elif a in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            new_args.append(a)\n",
    "    ast.args = new_args\n",
    "    return ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the implementation of `add_document`: the structure of `the_index` should be a `map` from words to lists of `(document_id, [list of positions])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document(doc):\n",
    "    '''\n",
    "    Add a document to the inverted index. Returns the document's ID\n",
    "    '''\n",
    "    global documents, docid_counter, the_index\n",
    "\n",
    "    # do not re-add the same document.\n",
    "    if doc in documents.values():\n",
    "        return documents[doc]\n",
    "    docid = docid_counter\n",
    "    documents[docid] = doc\n",
    "    docid_counter += 1\n",
    "    print(\"Adding document %s to inverted index with document ID %d\" % (doc, docid))\n",
    "    for pos, word in enumerate(tokenize_document(doc)):\n",
    "        # TODO for the assignment, implement this (remove the next line)\n",
    "        raise \"unimplemented\"\n",
    "    return docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.glob(corpuspath):\n",
    "    add_document(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the intersection of two posting lists with positional information (refer to the book). Be careful of the structure of the returned value: some of the code provided expects a list of tuples `(document_id, position_of_p1, position_of_p2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect_two(p1, p2, k):\n",
    "    '''\n",
    "    Intersect two posting lists according to pseudo-code in Introduction\n",
    "    to Information Retrieval, Figure 2.12\n",
    "    k is the max distance between the two words\n",
    "    Returns a list of tuples (document_id, position_of_p1, position_of_p2)\n",
    "    '''\n",
    "    answer = []\n",
    "    while p1 != [] and p2 != []:\n",
    "        # TODO for the assignment, implement this (remove the next line)\n",
    "        raise \"unimplemented\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs the intersection for a phrase query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect(words):\n",
    "    '''\n",
    "    Positionally intersect posting lists for a list of words\n",
    "    '''\n",
    "    assert(len(words) >= 0)\n",
    "    postings = [the_index[t] for t in words]\n",
    "    result = None\n",
    "    while len(postings) >= 2:\n",
    "        result = [(docid, [x[1] for x in g]) for docid, g\n",
    "                in itertools.groupby(positional_intersect_two(postings[-2], postings[-1], 1), key=lambda x: x[0])]\n",
    "        postings[-2:] = [result,]\n",
    "    return [x[0] for x in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the following test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(positional_intersect(['wit', 'folly', 'vanquished']) == [1])\n",
    "assert(positional_intersect(['send', 'some']) == [1, 8, 17, 20, 26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has the boolean query features you implemented last week (and some more). It will invoke the `positional_intersect` function for phrase terms in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_two(p1, p2):\n",
    "    '''\n",
    "    Intersect two posting lists according to pseudo-code in Introduction\n",
    "    to Information Retrieval, Figure 1.6\n",
    "    '''\n",
    "    answer = []\n",
    "    while p1 != [] and p2 != []:\n",
    "        if p1[0] == p2[0]:\n",
    "            answer.append(p1[0])\n",
    "            p1 = p1[1:]\n",
    "            p2 = p2[1:]\n",
    "        elif p1[0] < p2[0]:\n",
    "            p1 = p1[1:]\n",
    "        else:\n",
    "            p2 = p2[1:]\n",
    "    return answer\n",
    "\n",
    "def strip_positions(postings):\n",
    "    return (x[0] for x in postings)\n",
    "\n",
    "def negate(term):\n",
    "    if term in the_index.keys():\n",
    "        return sorted(set(documents.keys()) - set(strip_positions(the_index[term])))\n",
    "    else:\n",
    "        # What is the correct negation of a term not \n",
    "        return list(strip_positions(documents.keys()))\n",
    "\n",
    "def intersect(terms):\n",
    "    '''\n",
    "    Intersect posting lists for a list of terms\n",
    "    Take into consideration phrase queries\n",
    "    '''\n",
    "    def process_term(t):\n",
    "        if isinstance(t, list):\n",
    "            return positional_intersect(t)\n",
    "        elif isinstance(t, Operation) and t.op == 'NOT' and isinstance(t.args[0], list):\n",
    "            return sorted(set(documents.keys()) - set(positional_intersect(t.args[0])))\n",
    "        elif isinstance(t, str) and t.startswith('-'):\n",
    "            return negate(t[1:])\n",
    "        else:\n",
    "            assert(isinstance(t, str))\n",
    "            return list(strip_positions(the_index[t]))\n",
    "\n",
    "    postings = [process_term(t) for t in terms]\n",
    "    # calculate word frequencies and sort term,freq pairs in ascending\n",
    "    # order by frequency\n",
    "    freqs = sorted([ (p, len(p)) for p in postings ], key=lambda x: x[1] )\n",
    "    result = freqs[0][0]\n",
    "    del freqs[0]\n",
    "    while freqs != [] and freqs != []:\n",
    "        result = intersect_two(result, freqs[0][0])\n",
    "        del freqs[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a boolean query on the inverted index. We only support single\n",
    "    operator queries ATM.  This method returns a list of document ids\n",
    "    which satisfy the query in no particular order (i.e. the order in\n",
    "    which the documents were added most likely :)).\n",
    "    '''\n",
    "    # We use a generated parser to transform the query from a string to an\n",
    "    # AST.\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s'\\n\" % query, e)\n",
    "        return None\n",
    "\n",
    "    # We preprocess the AST to flatten commutative operations, such as\n",
    "    # sequences of ANDs. We also transform 'NOT <term>' arguments into\n",
    "    # '-<term>' to allow smarter processing of AND NOT and OR NOT.\n",
    "    flat = remove_stop_words(process_ast(ast))\n",
    "\n",
    "    args = []\n",
    "    # go through arguments and abort if we\n",
    "    # could not completely flatten the query\n",
    "    for arg in flat.args:\n",
    "        if isinstance(arg, Operation) and arg.op != 'NOT':\n",
    "            print(\"Cannot handle query '%s', aborting...\" % query)\n",
    "            return None\n",
    "        elif isinstance(arg, list):\n",
    "            # Assume it's a phrase\n",
    "            for w in arg:\n",
    "                assert(isinstance(w, str))\n",
    "            if any(w not in the_index.keys() for w in arg):\n",
    "                print(\"NOTE: Dropping phrase '%s' because no document contains it\" % ' '.join(arg))\n",
    "            else:\n",
    "                args.append(arg)\n",
    "        elif isinstance(arg, str) and not arg.startswith('-') and arg not in the_index.keys():\n",
    "            # Drop terms that don't occur in the vocabulary\n",
    "            print(\"NOTE: Dropping term '%s' because no document contains it\" % arg)\n",
    "        else:\n",
    "            args.append(arg)\n",
    "\n",
    "    if flat.op == 'OR':\n",
    "        results = set()\n",
    "        for arg in args:\n",
    "            if isinstance(arg, list):\n",
    "                results = results | set(positional_intersect(arg))\n",
    "            elif isinstance(arg, Operation) and arg.op == 'NOT' and isinstance(arg.args[0], list):\n",
    "                print(\"Query '%s' not supported\", query)\n",
    "                return None\n",
    "            elif arg.startswith('-'):\n",
    "                results = results | set(negate(arg[1:]))\n",
    "            else:\n",
    "                results = results | set(strip_positions(the_index[arg]))\n",
    "        return sorted(results)\n",
    "\n",
    "    elif flat.op == 'AND':\n",
    "        return intersect(args)\n",
    "\n",
    "    elif flat.op == 'NOT':\n",
    "        # handle case where we query for the negation of a term not in the\n",
    "        # vocabulary.\n",
    "        if len(args) == 0:\n",
    "            return list(documents.keys())\n",
    "        else:\n",
    "            assert(len(args) == 1)\n",
    "            if isinstance(args[0], list):\n",
    "                return sorted(set(documents.keys()) - set(positional_intersect(args[0])))\n",
    "            else:\n",
    "                return sorted(set(documents.keys()) - set(strip_positions(the_index[args[0]])))\n",
    "\n",
    "    elif flat.op == 'LOOKUP':\n",
    "        if len(args) == 0:\n",
    "            # we drop terms that are not in the vocabulary in the\n",
    "            # preprocessing loop, so handle the case where we query for a\n",
    "            # single term that is not in the vocabulary.\n",
    "            return []\n",
    "        else:\n",
    "            # in this case the query was a single term\n",
    "            assert(len(args) == 1)\n",
    "            if isinstance(args[0], list):\n",
    "                return positional_intersect(args[0])\n",
    "            else:\n",
    "                return list(strip_positions(the_index[args[0]]))\n",
    "    else:\n",
    "        print(\"Cannot handle query '%s', aborting...\" % query)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(docs):\n",
    "    '''\n",
    "    Helper function to convert a list of document IDs back to file names\n",
    "    '''\n",
    "    if not docs:\n",
    "        print(\"No documents found\")\n",
    "        print()\n",
    "        return\n",
    "    # If we got some results, print them\n",
    "    for doc in docs:\n",
    "        print('%d -> %s' % (doc, documents[doc]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the following queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_result(execute_query('\"a wit by folly vanquished\"'))\n",
    "print_result(execute_query('wit AND NOT \"else a wit\" AND sonnet'))\n",
    "print_result(execute_query('\"glooming peace\" OR Titus'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
