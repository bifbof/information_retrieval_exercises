{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a inverted index for n-words\n",
    "=========================================\n",
    "\n",
    "In this week's exercise we will look at enhanced inverted indices.\n",
    "The first form we look at is the biword index (cf. section 2.4.1 of the book)\n",
    "and its generalized form the n-word index.\n",
    "\n",
    "You can think of the biword index as an n-word index with `n = 2`.\n",
    "Similarly a triword index is identical to an n-word index with `n = 3`.\n",
    "Note that an n-word index with `n = 1` should be the same as a standard\n",
    "inverted index with respect to processing boolean queries.\n",
    "\n",
    "In this notebook we provide you with a mostly complete reference\n",
    "implementation for the standard inverted index which you had to implement last\n",
    "week.\n",
    "\n",
    "Your task is to modify `add_document()` to build an n-word index for a\n",
    "configurable n, and to modify `execute_query()` to be able to process phrase\n",
    "queries on the n-word index.\n",
    "A phrase query is a query such as \"Romans countrymen\" which should only return\n",
    "documents that contain this exact phrase.\n",
    "\n",
    "We updated the provided parser to support phrases in arbitrary boolean\n",
    "queries.\n",
    "Any phrase needs to be enclosed in double quotes for the query parser to be\n",
    "able to detect it.\n",
    "The output format for a phrase is a list of the individual terms that make up\n",
    "the phrase, in order.\n",
    "Below we show a few example queries containing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryparser import parse_query, process_ast\n",
    "print(process_ast(parse_query('\"Romans countrymen\"')))\n",
    "print(process_ast(parse_query('\"Romans countrymen lovers\"')))\n",
    "print(process_ast(parse_query('\"I think this\"')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make phrase queries more interesting we have modified the\n",
    "`tokenize_document()` function from last week to drop all the stop words\n",
    "listed in figure 2.5 in the book.\n",
    "If you're interested, you can acquire the list we use using the python snippet\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textutils import stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to remove stop words from your flattened queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(ast):\n",
    "    new_args = []\n",
    "    for a in ast.args:\n",
    "        if isinstance(a, list):\n",
    "            new_args.append([x for x in a if not x in stop_words])\n",
    "        elif isinstance(a, Operation):\n",
    "            new_args.append(remove_stop_words(a))\n",
    "        elif a[0] == '-' and a[1:] in stop_words:\n",
    "            pass\n",
    "        elif a in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            new_args.append(a)\n",
    "    ast.args = new_args\n",
    "    return ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you implement a general n-word index with `n` as a global\n",
    "variable that you can use to change between different indices.\n",
    "\n",
    "While we do not require you to be able to handle arbitrary boolean queries on\n",
    "your n-word index, think about how you would deal with queries that contain\n",
    "non-phrase operands in a more general purpose system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep the imports and global variables in a separate cell, so we can rerun\n",
    "# the code cell without loosing the contents of the index.\n",
    "from queryparser import parse_query, ParseException, process_ast, Operation\n",
    "import glob\n",
    "from textutils import tokenize_document\n",
    "# global variables defining the index\n",
    "documents = dict()\n",
    "the_index = dict()\n",
    "documentid_counter = 1\n",
    "# the path to the corpus\n",
    "corpuspath=\"../../shared/corpus/*.txt\"\n",
    "# the n for n-word\n",
    "N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document(path):\n",
    "    '''\n",
    "    Add a document to the inverted index. Return the document's document ID.\n",
    "    Remember the mapping from document ID to document in the `documents`\n",
    "    data structure.\n",
    "    '''\n",
    "    # make sure that we access the global variables we have defined\n",
    "    global the_index, documents, documentid_counter, n\n",
    "    # do not re-add the same document.\n",
    "    if path in documents.values():\n",
    "        # find and return document id for document which is already part of index.\n",
    "        for docid, doc in documents.items():\n",
    "            if doc == path:\n",
    "                return docid\n",
    "    docid = documentid_counter\n",
    "    documents[docid] = path\n",
    "    documentid_counter += 1\n",
    "    print(\"Adding '%s' to index\" % path)\n",
    "    currnword = []\n",
    "    for word in tokenize_document(path):\n",
    "        currnword.append(word)\n",
    "        # If we collected enough terms for an n-word phrase, map the n-word\n",
    "        # phrase to the document id in the inverted index\n",
    "        if len(currnword) == N:\n",
    "            p = ' '.join(currnword)\n",
    "            if p in the_index.keys() and the_index[p][-1] == docid:\n",
    "                # skip if n-word already associated with document\n",
    "                pass\n",
    "            else:\n",
    "                the_index.setdefault(p, []).append(docid)\n",
    "            # drop the first term of the n-word\n",
    "            currnword.pop(0)\n",
    "    return docid\n",
    "\n",
    "def negate(term):\n",
    "    '''\n",
    "    Negate postings list for `term`.  This is not feasible in a real-world\n",
    "    system, but we utilize this for the fallback execution which is fairly\n",
    "    naive.\n",
    "    '''\n",
    "    if term in the_index.keys():\n",
    "        return sorted(set(documents.keys()) - set(the_index[term]))\n",
    "    else:\n",
    "        return list(documents.keys())\n",
    "\n",
    "def intersect_two(p1, p2):\n",
    "    '''\n",
    "    Intersect two posting lists according to pseudo-code in Introduction\n",
    "    to Information Retrieval, Figure 1.6\n",
    "    '''\n",
    "    answer = []\n",
    "    while p1 != [] and p2 != []:\n",
    "        if p1[0] == p2[0]:\n",
    "            answer.append(p1[0])\n",
    "            p1 = p1[1:]\n",
    "            p2 = p2[1:]\n",
    "        elif p1[0] < p2[0]:\n",
    "            p1 = p1[1:]\n",
    "        else:\n",
    "            p2 = p2[1:]\n",
    "    return answer\n",
    "\n",
    "def intersect(terms):\n",
    "    '''\n",
    "    Intersect posting lists for a list of terms, according to pseudo-code\n",
    "    in Introduction to Information Retrieval, Figure 1.7\n",
    "    '''\n",
    "    postings = [ the_index[t] if not t.startswith('-') else\n",
    "                 negate(t[1:]) for t in terms ]\n",
    "    # calculate word frequencies and sort term,freq pairs in ascending\n",
    "    # order by frequency\n",
    "    freqs = sorted([ (t, len(p)) for t,p in zip(terms, postings) ], key=lambda x: x[1] )\n",
    "    terms, _ = map(list,zip(*freqs))\n",
    "    if terms[0].startswith('-'):\n",
    "        result = negate(terms[0][1:])\n",
    "    else:\n",
    "        result = the_index[terms[0]]\n",
    "    terms = terms[1:]\n",
    "    while terms != [] and result != []:\n",
    "        if terms[0].startswith('-'):\n",
    "            ps = negate(terms[0][1:])\n",
    "        else:\n",
    "            ps = the_index[terms[0]]\n",
    "        result = intersect_two(result, ps)\n",
    "        terms = terms[1:]\n",
    "    return result\n",
    "\n",
    "def phrase_and(phrase):\n",
    "    '''\n",
    "    Compute AND of n-word phrases given a longer phrase\n",
    "    '''\n",
    "    op = Operation('AND', [])\n",
    "    currphrase = []\n",
    "    for word in phrase:\n",
    "        currphrase.append(word)\n",
    "        if len(currphrase) == N:\n",
    "            # already compute nword phrase in form used for index\n",
    "            op.args.append(' '.join(currphrase))\n",
    "            currphrase.pop(0)\n",
    "    # Phrase-AND needs postfiltering\n",
    "    op.postfilter = True\n",
    "    return op\n",
    "\n",
    "def preprocess_query(flat):\n",
    "    # generally we don't need postfiltering\n",
    "    flat.postfilter = False\n",
    "    flat.complex = False\n",
    "    # go through arguments and recurse if we find another operation\n",
    "    for i in range(len(flat.args)):\n",
    "        arg = flat.args[i]\n",
    "        if isinstance(arg, Operation):\n",
    "            # as soon as we find a argument to the top-level operation\n",
    "            # which is not just a term, we fall back on the tree query\n",
    "            # execution strategy.\n",
    "            flat.args[i], _ = preprocess_query(flat)\n",
    "            flat.complex = True\n",
    "        elif isinstance(arg, list):\n",
    "            # Assume it is a phrase\n",
    "            for w in arg:\n",
    "                assert(isinstance(w, str))\n",
    "            # sanity checking\n",
    "            if len(arg) < N:\n",
    "                print(\"Unable to process phrases of length %d\" % len(arg))\n",
    "                print(\"Aborting query '%s'...\" % query)\n",
    "                return None\n",
    "            # reconstruct phrase to match ngram keys\n",
    "            if len(arg) > N:\n",
    "                tmp = phrase_and(arg)\n",
    "                # fold phrase AND with current level AND\n",
    "                if flat.op == 'AND':\n",
    "                    # drop phrases that are not in index\n",
    "                    args.extend([ a for a in tmp.args if a in the_index.keys() ])\n",
    "                    continue\n",
    "                elif flat.op == 'LOOKUP':\n",
    "                    flat = tmp\n",
    "                    flat.complex = False\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Query is now complex\")\n",
    "                    flat.args[i] = tmp\n",
    "                    flat.complex = True\n",
    "                    continue\n",
    "            p = ' '.join(arg)\n",
    "            if p not in the_index.keys():\n",
    "                print(\"NOTE: Dropping phrase '%s' because no document contains it\" % arg)\n",
    "            else:\n",
    "                flat.args[i] = ' '.join(arg)\n",
    "        else:\n",
    "            # nothing necessary\n",
    "            pass\n",
    "\n",
    "    return flat\n",
    "\n",
    "def postfilter(flat, result):\n",
    "    global documents\n",
    "    # This does not work for every case yet\n",
    "    real_result = []\n",
    "    # Reconstruct full phrase for postfiltering\n",
    "    fullphrase = ' '.join([ a.split(' ')[0] for a in flat.args ] + flat.args[-1].split(' ')[1:])\n",
    "    for doc in result:\n",
    "        currphrase = []\n",
    "        i = 0\n",
    "        for word in tokenize_document(documents[doc]):\n",
    "            currphrase.append(word)\n",
    "            if len(currphrase) == len(flat.args) + 1:\n",
    "                if fullphrase == ' '.join(currphrase):\n",
    "                    real_result.append(doc)\n",
    "                    break\n",
    "                currphrase.pop(0)\n",
    "            i += 1\n",
    "    return real_result\n",
    "\n",
    "def execute_query_tree(flat):\n",
    "    global the_index, documents\n",
    "    result = set()\n",
    "    if flat.op == 'AND':\n",
    "        result = set(documents.keys())\n",
    "    for arg in flat.args:\n",
    "        # execute subtree etc\n",
    "        if isinstance(arg, Operation):\n",
    "            temp = execute_query_tree(arg)\n",
    "        elif arg.startswith('-'):\n",
    "            temp = negate(arg[1:])\n",
    "        elif arg not in the_index.keys():\n",
    "            print(\"NOTE: dropping term '%s' because no document contains it\" % arg)\n",
    "        else:\n",
    "            temp = the_index[arg]\n",
    "\n",
    "        if flat.op == 'OR':\n",
    "            result = result | set(temp)\n",
    "        elif flat.op == 'AND':\n",
    "            result = result & set(temp)\n",
    "        elif flat.op == 'NOT':\n",
    "            assert(len(flat.args) == 1)\n",
    "            result = set(documents.keys()) - set(temp)\n",
    "\n",
    "    if flat.postfilter:\n",
    "        result = postfilter(flat, result)\n",
    "\n",
    "    return sorted(result)\n",
    "\n",
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a boolean query on the inverted index. We only support single\n",
    "    operator queries ATM.  This method returns a list of document ids\n",
    "    which satisfy the query in no particular order (i.e. the order in\n",
    "    which the documents were added most likely :)).\n",
    "    '''\n",
    "    # We use a generated parser to transform the query from a string to an\n",
    "    # AST.\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s'\\n\" % query, e)\n",
    "        return None\n",
    "\n",
    "    # We preprocess the AST to flatten commutative operations, such as\n",
    "    # sequences of ANDs. We also transform 'NOT <term>' arguments into\n",
    "    # '-<term>' to allow smarter processing of AND NOT and OR NOT.\n",
    "    flat = process_ast(ast)\n",
    "\n",
    "    # Feel free to remove this print() if you don't find it helpful.\n",
    "    print(\"Flat query repr:\", flat)\n",
    "\n",
    "    # preprocess flattened query, eliminate phrases, etc.\n",
    "    flat = preprocess_query(flat)\n",
    "\n",
    "    # Use recursive query processing for complex queries\n",
    "    if flat.complex:\n",
    "        return execute_query_tree(flat)\n",
    "\n",
    "    results = None\n",
    "    if flat.op == 'OR':\n",
    "        results = set()\n",
    "        for arg in flat.args:\n",
    "            if arg.startswith('-'):\n",
    "                print(\"OR NOT not handled (query: '%s'\" % query)\n",
    "                return None\n",
    "            else:\n",
    "                results = results | set(the_index[arg])\n",
    "        results = sorted(results)\n",
    "\n",
    "    elif flat.op == 'AND':\n",
    "        results = intersect(flat.args)\n",
    "\n",
    "    elif flat.op == 'LOOKUP':\n",
    "        assert(len(flat.args) == 1)\n",
    "        if flat.args[0] not in the_index.keys():\n",
    "            # single term query for term not in vocabulary, return empty list\n",
    "            # of document IDs\n",
    "            results = []\n",
    "        else:\n",
    "            # in this case the query was a single term\n",
    "            results = the_index[flat.args[0]]\n",
    "    else:\n",
    "        print(\"Cannot handle query '%s', aborting...\" % query)\n",
    "        return None\n",
    "\n",
    "    if flat.postfilter:\n",
    "        results = postfilter(flat, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_result(docs):\n",
    "    '''\n",
    "    Helper function to convert a list of document IDs back to file names\n",
    "    '''\n",
    "    if not docs:\n",
    "        print(\"No documents found\")\n",
    "        print()\n",
    "        return\n",
    "    # If we got some results, print them\n",
    "    for doc in docs:\n",
    "        print('%d -> %s' % (doc, documents[doc]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows us to build the index, and we run a test boolean query\n",
    "which the provided code should be able to answer satisfactorily.\n",
    "Because `add_document()` does not add documents that are already in the index,\n",
    "this cell can be run multiple times without adverse effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(corpuspath):\n",
    "    add_document(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the handout is correct, here is a query that the provided code\n",
    "should be able to answer.\n",
    "Note that this query is not trivially answerable on a n-word index for n > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_result(execute_query(\"Brutus AND Calpurnia\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a list of example phrase queries that your n-word index\n",
    "should be able to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected result: the_tragedy_of_julius_caesar.txt\n",
    "print_result(execute_query('\"Romans countrymen\"')) \n",
    "# expected result: the_tragedy_of_julius_caesar.txt\n",
    "print_result(execute_query('\"Romans countrymen lovers\"'))\n",
    "# expected result:\n",
    "#   king_henry_the_eighth.txt\n",
    "#   much_ado_about_nothing.txt\n",
    "#   the_first_part_of_henry_the_sixth.txt\n",
    "#   the_first_part_of_king_henry_the_fourth.txt\n",
    "#   the_life_of_timon_of_athens.txt\n",
    "#   the_second_part_of_king_henry_the_sixth.txt\n",
    "#   the_tragedy_of_king_lear.txt\n",
    "#   the_tragedy_of_othello_moor_of_venice.txt\n",
    "#   the_winters_tale.txt\n",
    "print_result(execute_query('\"I think this\"'))\n",
    "\n",
    "print_result(execute_query('\"I think this\" OR \"Romans countrymen lovers\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just implement phrase queries on your n-word index as discussed in the\n",
    "book in section 2.4.1, you will notice that the query \"I think this\" returns\n",
    "a number of documents that do not actually contain the phrase \"I think this\"\n",
    "when we query a 2-word index.\n",
    "\n",
    "A way to deal with this is to post-process the results of the query execution\n",
    "engine to drop documents which only contain partial phrases.\n",
    "\n",
    "I decide to implement the postprocessing by calling out to the [`grep`\n",
    "tool](https://en.wikipedia.org/wiki/Grep).\n",
    "What are potential problems with this postprocessing strategy?\n",
    "\n",
    "Answer:\n",
    " * Plain grep will not skip stop words\n",
    " * Plain grep will not match phrases across punctuation or newlines\n",
    "\n",
    "Optional exercises\n",
    "------------------\n",
    "\n",
    "If you didn't feel challenged by this week's work, you can try your hand at\n",
    "doing (some of) the following implementation work:\n",
    "\n",
    " * implement a postprocessing step which actually correctly filters the\n",
    "   results\n",
    "     * See `postfilter`.\n",
    " * Improve your query execution engine to handle phrases in conjunction with\n",
    "   boolean operators.\n",
    "     * See `preprocess_query` and `execute_query_tree`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
