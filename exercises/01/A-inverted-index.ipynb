{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hollywood-examination",
   "metadata": {},
   "source": [
    "Implementing a simple inverted index\n",
    "====================================\n",
    "\n",
    "In this exercise you will implement a simple inverted index in python.\n",
    "\n",
    "We provide a boolean query parser for you. You can use the parser as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c803f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "structured-passage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AST({'fst': AST({'fst': AST({'fst': 'Caesar', 'op': None, 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@17, rule='notexpr', pos=0, endpos=6, line=0, endline=0)}), 'op': 'AND', 'lst': AST({'fst': 'Brutus', 'op': None, 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@17, rule='notexpr', pos=11, endpos=17, line=0, endline=1)}), 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@17, rule='andexpr', pos=0, endpos=17, line=0, endline=1)}), 'lst': None, 'op': None, 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@17, rule='expr', pos=0, endpos=17, line=0, endline=1)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from queryparser import parse_query\n",
    "parse_query('Caesar AND Brutus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-charter",
   "metadata": {},
   "source": [
    "Note that the result of `parse_query` is an abstract syntax tree (AST).  We\n",
    "provide another method `process_ast`, which will traverse the AST and produce\n",
    "a flattened version for some queries.  The parser can deal with any valid\n",
    "boolean query which uses the operators `AND`, `OR`, and `NOT`. However your\n",
    "solution will not have to be able to handle all the possible queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "danish-superior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AND: args=['Caesar', 'Brutus']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from queryparser import parse_query, process_ast\n",
    "process_ast(parse_query('Caesar AND Brutus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-proxy",
   "metadata": {},
   "source": [
    "To handle errors in the query parser, you can catch `ParseException`.  For\n",
    "example, the query `Clown AND OR Circus` is not a well-formed boolean query.\n",
    "To not have your code crash with an unhandled exception you can do something\n",
    "like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse query:\n",
      " (1:13) \"OR\" is a reserved word :\n",
      "Clown AND OR Circus\n",
      "            ^\n",
      "word\n",
      "primaryexpr\n",
      "notexpr\n",
      "andexpr\n",
      "expr\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "from queryparser import ParseException\n",
    "try:\n",
    "    ast = parse_query('Clown AND OR Circus')\n",
    "except ParseException as e:\n",
    "    print(\"Failed to parse query:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-wallace",
   "metadata": {},
   "source": [
    "For the purposes of this exercise we provide you with the collected works of\n",
    "Shakespeare as retrieved from\n",
    "[gutenberg.org](http://www.gutenberg.org/files/100/100-0.txt) and split up\n",
    "into individual text files which you can find in `shared/corpus` in the root\n",
    "directory of your Jupyter account (which is your home directory on that\n",
    "machine).\n",
    "\n",
    "In a future exercise, we will ask you to implement an algorithm for splitting\n",
    "up such a text file into smaller documents yourselves, but for this week, we\n",
    "have done the preprocessing for you.\n",
    "\n",
    "To iterate over all the text files, for example to add them to your inverted\n",
    "index, you can use python's `glob` library which allows you to list files\n",
    "using regular Unix shell [glob syntax](https://en.wikipedia.org/wiki/Glob_(programming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "global-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../shared/corpus\\alls_well_that_ends_well.txt\n",
      "../../shared/corpus\\as_you_like_it.txt\n",
      "../../shared/corpus\\a_lovers_complaint.txt\n",
      "../../shared/corpus\\a_midsummer_nights_dream.txt\n",
      "../../shared/corpus\\cymbeline.txt\n",
      "../../shared/corpus\\king_henry_the_eighth.txt\n",
      "../../shared/corpus\\king_john.txt\n",
      "../../shared/corpus\\king_richard_the_second.txt\n",
      "../../shared/corpus\\king_richard_the_third.txt\n",
      "../../shared/corpus\\loves_labours_lost.txt\n",
      "../../shared/corpus\\measure_for_measure.txt\n",
      "../../shared/corpus\\much_ado_about_nothing.txt\n",
      "../../shared/corpus\\pericles_prince_of_tyre.txt\n",
      "../../shared/corpus\\the_comedy_of_errors.txt\n",
      "../../shared/corpus\\the_first_part_of_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_first_part_of_king_henry_the_fourth.txt\n",
      "../../shared/corpus\\the_history_of_troilus_and_cressida.txt\n",
      "../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "../../shared/corpus\\the_life_of_timon_of_athens.txt\n",
      "../../shared/corpus\\the_merchant_of_venice.txt\n",
      "../../shared/corpus\\the_merry_wives_of_windsor.txt\n",
      "../../shared/corpus\\the_passionate_pilgrim.txt\n",
      "../../shared/corpus\\the_phoenix_and_the_turtle.txt\n",
      "../../shared/corpus\\the_rape_of_lucrece.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_fourth.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_sonnets.txt\n",
      "../../shared/corpus\\the_taming_of_the_shrew.txt\n",
      "../../shared/corpus\\the_tempest.txt\n",
      "../../shared/corpus\\the_third_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt\n",
      "../../shared/corpus\\the_tragedy_of_coriolanus.txt\n",
      "../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "../../shared/corpus\\the_tragedy_of_king_lear.txt\n",
      "../../shared/corpus\\the_tragedy_of_macbeth.txt\n",
      "../../shared/corpus\\the_tragedy_of_othello_moor_of_venice.txt\n",
      "../../shared/corpus\\the_tragedy_of_romeo_and_juliet.txt\n",
      "../../shared/corpus\\the_tragedy_of_titus_andronicus.txt\n",
      "../../shared/corpus\\the_two_gentlemen_of_verona.txt\n",
      "../../shared/corpus\\the_two_noble_kinsmen.txt\n",
      "../../shared/corpus\\the_winters_tale.txt\n",
      "../../shared/corpus\\twelfth_night_or_what_you_will.txt\n",
      "../../shared/corpus\\venus_and_adonis.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for file in glob.glob(\"../../shared/corpus/*.txt\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-documentation",
   "metadata": {},
   "source": [
    "Now that we have a way to parse a boolean query, and a corpus of documents,\n",
    "what is left to do is to implement a simple inverted index.  For this exercise\n",
    "you can assume that the dictionary, and the postings fit into main memory.\n",
    "\n",
    "We also provide you with a very simple method which handles both the\n",
    "tokenizing of a document and preprocessing of the resulting tokens.  This\n",
    "method will not do much linguistic preprocessing but rather just strips each\n",
    "token of special characters such as quotes, punctuation marks, etc.\n",
    "\n",
    "We will discuss linguistic preprocessing in more details in future exercises.\n",
    "\n",
    "Below is a very simple example of how to use the `tokenize_document` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latin-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE\n",
      "PHOENIX\n",
      "AND\n",
      "THE\n",
      "TURTLE\n",
      "William\n",
      "Shakespeare\n",
      "Let\n",
      "bird\n",
      "loudest\n",
      "lay\n",
      "On\n",
      "sole\n",
      "Arabian\n",
      "tree\n",
      "Herald\n",
      "sad\n",
      "trumpet\n",
      "be\n",
      "To\n",
      "whose\n",
      "sound\n",
      "chaste\n",
      "wings\n",
      "obey\n",
      "But\n",
      "thou\n",
      "shrieking\n",
      "harbinger\n",
      "Foul\n",
      "precurrer\n",
      "fiend\n",
      "Augur\n",
      "fever’s\n",
      "end\n",
      "To\n",
      "this\n",
      "troop\n",
      "come\n",
      "thou\n",
      "not\n",
      "near\n",
      "From\n",
      "this\n",
      "session\n",
      "interdict\n",
      "Every\n",
      "fowl\n",
      "tyrant\n",
      "wing\n",
      "Save\n",
      "eagle\n",
      "feather’d\n",
      "king\n",
      "Keep\n",
      "obsequy\n",
      "so\n",
      "strict\n",
      "Let\n",
      "priest\n",
      "surplice\n",
      "white\n",
      "That\n",
      "defunctive\n",
      "music\n",
      "can\n",
      "Be\n",
      "deathdivining\n",
      "swan\n",
      "Lest\n",
      "requiem\n",
      "lack\n",
      "his\n",
      "right\n",
      "And\n",
      "thou\n",
      "trebledated\n",
      "crow\n",
      "That\n",
      "thy\n",
      "sable\n",
      "gender\n",
      "mak’st\n",
      "With\n",
      "breath\n",
      "thou\n",
      "giv’st\n",
      "tak’st\n",
      "’Mongst\n",
      "our\n",
      "mourners\n",
      "shalt\n",
      "thou\n",
      "go\n",
      "Here\n",
      "anthem\n",
      "doth\n",
      "commence\n",
      "Love\n",
      "constancy\n",
      "dead\n",
      "Phoenix\n",
      "turtle\n",
      "fled\n",
      "In\n",
      "mutual\n",
      "flame\n",
      "hence\n",
      "So\n",
      "they\n",
      "lov’d\n",
      "love\n",
      "twain\n",
      "Had\n",
      "essence\n",
      "but\n",
      "one\n",
      "Two\n",
      "distincts\n",
      "division\n",
      "none\n",
      "Number\n",
      "there\n",
      "love\n",
      "slain\n",
      "Hearts\n",
      "remote\n",
      "yet\n",
      "not\n",
      "asunder\n",
      "Distance\n",
      "no\n",
      "space\n",
      "seen\n",
      "’Twixt\n",
      "this\n",
      "turtle\n",
      "his\n",
      "queen\n",
      "But\n",
      "them\n",
      "wonder\n",
      "So\n",
      "between\n",
      "them\n",
      "love\n",
      "did\n",
      "shine\n",
      "That\n",
      "turtle\n",
      "saw\n",
      "his\n",
      "right\n",
      "Flaming\n",
      "phoenix’\n",
      "sight\n",
      "Either\n",
      "other’s\n",
      "mine\n",
      "Property\n",
      "thus\n",
      "appalled\n",
      "That\n",
      "self\n",
      "not\n",
      "same\n",
      "Single\n",
      "nature’s\n",
      "double\n",
      "name\n",
      "Neither\n",
      "two\n",
      "nor\n",
      "one\n",
      "called\n",
      "Reason\n",
      "itself\n",
      "confounded\n",
      "Saw\n",
      "division\n",
      "grow\n",
      "together\n",
      "To\n",
      "themselves\n",
      "yet\n",
      "either\n",
      "neither\n",
      "Simple\n",
      "so\n",
      "well\n",
      "compounded\n",
      "That\n",
      "cried\n",
      "How\n",
      "true\n",
      "twain\n",
      "Seemeth\n",
      "this\n",
      "concordant\n",
      "one\n",
      "Love\n",
      "hath\n",
      "reason\n",
      "reason\n",
      "none\n",
      "If\n",
      "what\n",
      "parts\n",
      "can\n",
      "so\n",
      "remain\n",
      "Whereupon\n",
      "made\n",
      "this\n",
      "threne\n",
      "To\n",
      "phoenix\n",
      "dove\n",
      "Cosupremes\n",
      "stars\n",
      "love\n",
      "As\n",
      "chorus\n",
      "their\n",
      "tragic\n",
      "scene\n",
      "THRENOS\n",
      "Beauty\n",
      "truth\n",
      "rarity\n",
      "Grace\n",
      "all\n",
      "simplicity\n",
      "Here\n",
      "enclos’d\n",
      "cinders\n",
      "lie\n",
      "Death\n",
      "now\n",
      "phoenix’\n",
      "nest\n",
      "And\n",
      "turtle’s\n",
      "loyal\n",
      "breast\n",
      "To\n",
      "eternity\n",
      "doth\n",
      "rest\n",
      "Leaving\n",
      "no\n",
      "posterity—\n",
      "’Twas\n",
      "not\n",
      "their\n",
      "infirmity\n",
      "It\n",
      "married\n",
      "chastity\n",
      "Truth\n",
      "may\n",
      "seem\n",
      "but\n",
      "cannot\n",
      "be\n",
      "Beauty\n",
      "brag\n",
      "but\n",
      "’tis\n",
      "not\n",
      "she\n",
      "Truth\n",
      "beauty\n",
      "buried\n",
      "be\n",
      "To\n",
      "this\n",
      "urn\n",
      "let\n",
      "those\n",
      "repair\n",
      "That\n",
      "either\n",
      "true\n",
      "or\n",
      "fair\n",
      "For\n",
      "these\n",
      "dead\n",
      "birds\n",
      "sigh\n",
      "prayer\n"
     ]
    }
   ],
   "source": [
    "from textutils import tokenize_document\n",
    "for token in tokenize_document('../../shared/corpus/the_phoenix_and_the_turtle.txt'):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-entrance",
   "metadata": {},
   "source": [
    "Now all that is left to do is to implement some code which will create an\n",
    "inverted index.  We give you a very simple skeleton which outlines the\n",
    "implementation work that you will have to do.\n",
    "\n",
    "In the index we want to refer to documents by a document ID.\n",
    "We define variables `documents` which we will assume to be the datastructure\n",
    "mapping document IDs to documents and `the_index` which will be the variable\n",
    "holding the inverted index.\n",
    "\n",
    "Note that while we currently initialize `the_index` to `None`, you will have\n",
    "to choose a data structure to represent the inverted index, and initialize the\n",
    "variable accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "maritime-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dict()\n",
    "the_index = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-perception",
   "metadata": {},
   "source": [
    "The easiest way to do this for now, is to simply have a counter which we\n",
    "increment every time we add a document to the index.  In order to be able to\n",
    "produce human-readable results you will need to also keep track of which\n",
    "document ID refers to which document.  The function we provide to print\n",
    "results expects that `documents` is a `dict` mapping document IDs to document\n",
    "file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "independent-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentid_counter = 1\n",
    "def add_document(path):\n",
    "    '''\n",
    "    Add a document to the inverted index. Return the document's document ID.\n",
    "    Remember the mapping from document ID to document in the `documents`\n",
    "    data structure.\n",
    "    '''\n",
    "    # make sure that we access the global variables we have defined\n",
    "    global the_index, documents, documentid_counter\n",
    "    docid = documentid_counter\n",
    "    documentid_counter += 1\n",
    "    print(\"Adding '%s' to index\" % path)\n",
    "    documents[docid] = path\n",
    "    for token in tokenize_document(path):\n",
    "        posting_list = the_index.get(token, None)\n",
    "        if posting_list is None:\n",
    "            the_index[token] = [docid]\n",
    "        elif posting_list[-1] != docid:\n",
    "            posting_list.append(docid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-oklahoma",
   "metadata": {},
   "source": [
    "Assuming you have implemented `add_document` correctly you should be able to\n",
    "create an inverted index as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "measured-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding '../../shared/corpus\\alls_well_that_ends_well.txt' to index\n",
      "Adding '../../shared/corpus\\as_you_like_it.txt' to index\n",
      "Adding '../../shared/corpus\\a_lovers_complaint.txt' to index\n",
      "Adding '../../shared/corpus\\a_midsummer_nights_dream.txt' to index\n",
      "Adding '../../shared/corpus\\cymbeline.txt' to index\n",
      "Adding '../../shared/corpus\\king_henry_the_eighth.txt' to index\n",
      "Adding '../../shared/corpus\\king_john.txt' to index\n",
      "Adding '../../shared/corpus\\king_richard_the_second.txt' to index\n",
      "Adding '../../shared/corpus\\king_richard_the_third.txt' to index\n",
      "Adding '../../shared/corpus\\loves_labours_lost.txt' to index\n",
      "Adding '../../shared/corpus\\measure_for_measure.txt' to index\n",
      "Adding '../../shared/corpus\\much_ado_about_nothing.txt' to index\n",
      "Adding '../../shared/corpus\\pericles_prince_of_tyre.txt' to index\n",
      "Adding '../../shared/corpus\\the_comedy_of_errors.txt' to index\n",
      "Adding '../../shared/corpus\\the_first_part_of_henry_the_sixth.txt' to index\n",
      "Adding '../../shared/corpus\\the_first_part_of_king_henry_the_fourth.txt' to index\n",
      "Adding '../../shared/corpus\\the_history_of_troilus_and_cressida.txt' to index\n",
      "Adding '../../shared/corpus\\the_life_of_king_henry_the_fifth.txt' to index\n",
      "Adding '../../shared/corpus\\the_life_of_timon_of_athens.txt' to index\n",
      "Adding '../../shared/corpus\\the_merchant_of_venice.txt' to index\n",
      "Adding '../../shared/corpus\\the_merry_wives_of_windsor.txt' to index\n",
      "Adding '../../shared/corpus\\the_passionate_pilgrim.txt' to index\n",
      "Adding '../../shared/corpus\\the_phoenix_and_the_turtle.txt' to index\n",
      "Adding '../../shared/corpus\\the_rape_of_lucrece.txt' to index\n",
      "Adding '../../shared/corpus\\the_second_part_of_king_henry_the_fourth.txt' to index\n",
      "Adding '../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt' to index\n",
      "Adding '../../shared/corpus\\the_sonnets.txt' to index\n",
      "Adding '../../shared/corpus\\the_taming_of_the_shrew.txt' to index\n",
      "Adding '../../shared/corpus\\the_tempest.txt' to index\n",
      "Adding '../../shared/corpus\\the_third_part_of_king_henry_the_sixth.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_coriolanus.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_julius_caesar.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_king_lear.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_macbeth.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_othello_moor_of_venice.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_romeo_and_juliet.txt' to index\n",
      "Adding '../../shared/corpus\\the_tragedy_of_titus_andronicus.txt' to index\n",
      "Adding '../../shared/corpus\\the_two_gentlemen_of_verona.txt' to index\n",
      "Adding '../../shared/corpus\\the_two_noble_kinsmen.txt' to index\n",
      "Adding '../../shared/corpus\\the_winters_tale.txt' to index\n",
      "Adding '../../shared/corpus\\twelfth_night_or_what_you_will.txt' to index\n",
      "Adding '../../shared/corpus\\venus_and_adonis.txt' to index\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for file in glob.glob('../../shared/corpus/*.txt'):\n",
    "    add_document(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1ddb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 19, 27, 29, 31, 36, 41]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_index[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-poison",
   "metadata": {},
   "source": [
    "Now you need to implement a method to process boolean queries.  For this\n",
    "exercise it is enough if your query processing can handle simple boolean\n",
    "query with at most one operation.  Below is a selection of queries your\n",
    "implementation should be able to handle.\n",
    "\n",
    "So you do not have to walk the AST you can use the method `process_ast` which\n",
    "was mentioned earlier to transform the AST into a flattened representation.\n",
    "Additionally we provide a method `operation_is_complex`, which will return\n",
    "`True` if the query is more complex than what we reasonably expect you to\n",
    "handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "forward-ghana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST representation: AST({'fst': AST({'fst': AST({'fst': 'Brutus', 'op': None, 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@20, rule='notexpr', pos=0, endpos=6, line=0, endline=0)}), 'op': 'AND', 'lst': AST({'fst': 'Calpurnia', 'op': None, 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@20, rule='notexpr', pos=11, endpos=20, line=0, endline=1)}), 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@20, rule='andexpr', pos=0, endpos=20, line=0, endline=1)}), 'lst': None, 'op': None, 'parseinfo': ParseInfo(buffer=BooleanQueryBuffer@20, rule='expr', pos=0, endpos=20, line=0, endline=1)})\n",
      "Flattened representation: AND: args=['Brutus', 'Calpurnia']\n",
      "AND\n",
      "['Brutus', 'Calpurnia']\n"
     ]
    }
   ],
   "source": [
    "from queryparser import process_ast\n",
    "ast = parse_query('Brutus AND Calpurnia')\n",
    "print(\"AST representation:\", ast)\n",
    "flat = process_ast(ast)\n",
    "print(\"Flattened representation:\", flat)\n",
    "print(flat.op)\n",
    "print(flat.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-backing",
   "metadata": {},
   "source": [
    "As you can see, it is easier to extract the operation and operands from the\n",
    "flat representation.  The flattened representation is composed from\n",
    "`Operation` objects, and strings.\n",
    "An `Operation` object is a representation of a boolean query operation.  The\n",
    "`op` field represents the operation type which should be one of `AND`, `OR`,\n",
    "`NOT`, or `LOOKUP`.\n",
    "The `args` field is a list of one or more arguments for the operation.  `NOT`\n",
    "and `LOOKUP` operations are expected to have a argument list of length one.\n",
    "`AND`, and `OR` can have arbitrarily long argument lists, but the argument\n",
    "list for such an operation should contain at least two elements.\n",
    "For `AND` and `OR`, we represent operands of the form 'NOT <term>' as the\n",
    "string '-<term>' to allow more efficient execution of queries of the form 'a\n",
    "AND NOT b' and 'a OR NOT b'.\n",
    "\n",
    "Before directly diving into the full query processing code, you will have to\n",
    "implement methods that compute the intersection and union of two postings\n",
    "lists.  We give you method declarations for these operations, but if you need\n",
    "to change the method arguments feel free to do so.\n",
    "\n",
    "While we do not give hard requirements on the method arguments for either\n",
    "operation, you should probably consider making it a requirement that the\n",
    "postings lists which you pass to `intersect` and `union` are sorted, as that\n",
    "will make implementing the functionality significantly easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "enhanced-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(p1, p2):\n",
    "    '''\n",
    "    Method to compute the intersection of two postings lists.  Takes two\n",
    "    postings lists as arguments and returns the intersection.\n",
    "    '''\n",
    "    answer = []\n",
    "    iter_p1 = iter(p1)\n",
    "    iter_p2 = iter(p2)\n",
    "    doc1 = next(iter_p1, None)\n",
    "    doc2 = next(iter_p2, None)\n",
    "    while doc1 is not None and doc2 is not None:\n",
    "        if doc1 == doc2:\n",
    "            answer.append(doc1)\n",
    "            doc1 = next(iter_p1, None)\n",
    "            doc2 = next(iter_p2, None)\n",
    "        elif doc1 < doc2:\n",
    "            doc1 = next(iter_p1, None)\n",
    "        else:\n",
    "            doc2 = next(iter_p2, None)  \n",
    "    return answer\n",
    "\n",
    "def union(p1, p2):\n",
    "    '''\n",
    "    Method to compute the union of two postings lists.  Takes two\n",
    "    postings lists as arguments and returns the union.\n",
    "    '''\n",
    "    answer = []\n",
    "    iter_p1 = iter(p1)\n",
    "    iter_p2 = iter(p2)\n",
    "    doc1 = next(iter_p1, None)\n",
    "    doc2 = next(iter_p2, None)\n",
    "    while doc1 is not None and doc2 is not None:\n",
    "        if doc1 == doc2:\n",
    "            answer.append(doc1)\n",
    "            doc1 = next(iter_p1, None)\n",
    "            doc2 = next(iter_p2, None)\n",
    "        elif doc1 < doc2:\n",
    "            answer.append(doc1)\n",
    "            doc1 = next(iter_p1, None)\n",
    "        else:\n",
    "            answer.append(doc2)\n",
    "            doc2 = next(iter_p2, None)\n",
    "    while doc1 is not None:\n",
    "        answer.append(doc1)\n",
    "        doc1 = next(iter_p1, None)\n",
    "    while doc2 is not None:\n",
    "        answer.append(doc2)\n",
    "        doc2 = next(iter_p2, None)\n",
    "    return answer\n",
    "\n",
    "def subtraction(p1, p2):\n",
    "    i_p1 = 0\n",
    "    i_p2 = 0\n",
    "    subtraction_t =[]\n",
    "    while i_p1 < len(p1) and i_p2 < len(p2):\n",
    "        if p1[i_p1] > p2[i_p2]:\n",
    "            i_p2 += 1\n",
    "        elif p1[i_p1] == p2[i_p2]:\n",
    "            i_p1 += 1\n",
    "            i_p2 += 1\n",
    "        else:  # p1 < p2\n",
    "            subtraction_t.append(p1[i_p1])\n",
    "            i_p1 += 1\n",
    "    if i_p1 < len(p1):\n",
    "        subtraction_t.extend(p1[i_p1:])\n",
    "    return subtraction_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-exemption",
   "metadata": {},
   "source": [
    "Below there are a few example calls to `intersect` and `union` which you can\n",
    "use to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "equal-parks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n",
      "[]\n",
      "[1, 6]\n",
      "[]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 4]\n",
      "[1, 2, 4, 5, 6]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(intersect([1,2,3,6], [1,3,4])) # should produce [1,3]\n",
    "print(intersect([1,6], [3,4])) # should produce []\n",
    "print(intersect([1,2,6], [1,6])) # should produce [1,6]\n",
    "print(intersect([], [1,3])) # should produce []\n",
    "\n",
    "print(union([1,2], [3,4])) # should produce [1,2,3,4]\n",
    "print(union([1,2], [1,2,4])) # should produce [1,2,4]\n",
    "print(union([1,2,6], [2,4,5])) # should produce [1,2,4,5,6]\n",
    "print(union([], [1,3])) # should produce [1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-absorption",
   "metadata": {},
   "source": [
    "Now you should be fairly well-equipped to implement a boolean query processor.\n",
    "Feel free to split your implementation into multiple methods if you find that\n",
    "`execute_query` is getting too big for your taste.\n",
    "\n",
    "Note that we expect you to implement intersections and unions of more than two\n",
    "terms in the way discussed in the lecture and the book.\n",
    "\n",
    "Queries that are conjunctions where all terms are negated (i.e.\n",
    "`NOT Brutus AND NOT Calpurnia`) and queries that are disjunction where\n",
    "at least one term is negated (i.e. `Brutus OR NOT Calpurnia OR Hamlet`)\n",
    "are often not implemented in real-world, large-scale, general purpose systems.\n",
    "We do not expect you to implement them either, your code can just return `None`\n",
    "in these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "armed-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryparser import operation_is_complex\n",
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a boolean query on the inverted index. This method should return a\n",
    "    list of document ids which satisfy the query. The list does not need to be\n",
    "    in a particular order.\n",
    "    '''\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s':\\n\" % query, e)\n",
    "\n",
    "    # flatten query AST\n",
    "    flat = process_ast(ast)\n",
    "\n",
    "    if operation_is_complex(flat):\n",
    "        print(\"Complex query '%s'\" % query)\n",
    "        return None\n",
    "\n",
    "    # TODO: implement query processing\n",
    "    print(\"Processing\", flat)\n",
    "    p1 = flat.args[0]\n",
    "    p1_t = the_index.get(p1, [])\n",
    "    for p2 in flat.args[1:]:\n",
    "        if flat.op == \"AND\":\n",
    "            if p2.startswith(\"-\"):\n",
    "                p2_t = the_index.get(p2[1:], [])\n",
    "                p1_t = subtraction(p1_t, p2_t)\n",
    "            else:\n",
    "                p2_t = the_index.get(p2, [])\n",
    "                p1_t = intersect(p1_t, p2_t)\n",
    "        elif flat.op == \"OR\":\n",
    "            if p2.startswith(\"-\"):\n",
    "                raise ValueError(\"OR NOT is not allowed.\")\n",
    "            else:\n",
    "                p2_t = the_index.get(p2, [])\n",
    "                p1_t = union(p1_t, p2_t)      \n",
    "    return p1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b491110",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d96c2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queryparser import Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "765738a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(term):\n",
    "    \"\"\"\n",
    "    Lookup term in the_index\n",
    "    \n",
    "    Inputs\n",
    "    term : str or list\n",
    "        if str lookup term in index else just return\n",
    "\n",
    "    Returns\n",
    "        bool, list\n",
    "        boolean if NOT operation is attached, list for posting list.\n",
    "    \"\"\"\n",
    "    not_operation = False\n",
    "    if isinstance(term, str):\n",
    "        if term.startswith(\"-\"):\n",
    "            term = term[1:]\n",
    "            not_operation = True\n",
    "        return not_operation, the_index.get(term, [])\n",
    "    return not_operation, term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6460c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "db8bb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query_tree(tree):\n",
    "    \"\"\"\n",
    "    Execute a booloean query index on the inverted index.\n",
    "    \n",
    "    Inputs\n",
    "    tree : Operation\n",
    "\n",
    "    Returns\n",
    "        list\n",
    "        list of documents\n",
    "    \"\"\"\n",
    "    # recursive flattening of the tree depth first\n",
    "    for i, arg in enumerate(tree.args):\n",
    "        if isinstance(arg, Operation):\n",
    "            tree.args[i] = execute_query_tree(arg)  # list in args\n",
    "    # now the query should be flat\n",
    "    _, p1 = lookup(tree.args[0])\n",
    "    for term in tree.args[1:]:\n",
    "        _not, p2 = lookup(term)\n",
    "        if tree.op == \"AND\":\n",
    "            if _not:\n",
    "                p1 = subtraction(p1, p2)\n",
    "            else:\n",
    "                p1 = intersect(p1, p2)\n",
    "        elif tree.op == \"OR\":\n",
    "            if _not:\n",
    "                raise NotImplementedError(\"NOT OR is not implemented\")\n",
    "            else:\n",
    "                p1 = union(p1, p2)\n",
    "        else:\n",
    "            # LOOKUP does not reach this stage as tree.args[0] should be empty\n",
    "            # not the cleanest but enough\n",
    "            raise ValueError(\"Do not know operation {}\".format(tree.op))\n",
    "    return p1    \n",
    "            \n",
    "    \n",
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a boolean query on the inverted index. This method should return a\n",
    "    list of document ids which satisfy the query. The list does not need to be\n",
    "    in a particular order.\n",
    "    '''\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s':\\n\" % query, e)\n",
    "\n",
    "    # flatten query AST\n",
    "    flat = process_ast(ast)\n",
    "    print(\"Processing\", flat)\n",
    "    return execute_query_tree(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-karaoke",
   "metadata": {},
   "source": [
    "If you can process all the queries given below, you have made a good start on\n",
    "building an inverted index and boolean query processor.\n",
    "A good way to improve your query processing is to directly handle `AND NOT`.\n",
    "You can either try to extend the `intersect` method which you implemented\n",
    "previously, or you can implement `AND NOT` in a separate method.\n",
    "\n",
    "In order to get human readable output, the following function translates\n",
    "document IDs back to file names, assuming you've correctly populated the\n",
    "`documents` dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "recognized-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(docs):\n",
    "    if not docs:\n",
    "        print(\"No documents found\")\n",
    "        print()\n",
    "        return\n",
    "    # If we got some results, print them\n",
    "    for doc in docs:\n",
    "        print(documents[doc])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "outside-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LOOKUP: args=['Caesar']\n",
      "../../shared/corpus\\alls_well_that_ends_well.txt\n",
      "../../shared/corpus\\cymbeline.txt\n",
      "../../shared/corpus\\king_richard_the_third.txt\n",
      "../../shared/corpus\\measure_for_measure.txt\n",
      "../../shared/corpus\\the_first_part_of_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_third_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt\n",
      "../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "../../shared/corpus\\the_tragedy_of_macbeth.txt\n",
      "../../shared/corpus\\the_tragedy_of_othello_moor_of_venice.txt\n",
      "\n",
      "Processing LOOKUP: args=['hello']\n",
      "No documents found\n",
      "\n",
      "Processing AND: args=['Brutus', 'Calpurnia']\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "\n",
      "Processing OR: args=['Brutus', 'Hamlet']\n",
      "../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "../../shared/corpus\\the_merchant_of_venice.txt\n",
      "../../shared/corpus\\the_rape_of_lucrece.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt\n",
      "../../shared/corpus\\the_tragedy_of_coriolanus.txt\n",
      "../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "../../shared/corpus\\the_tragedy_of_titus_andronicus.txt\n",
      "\n",
      "Processing AND: args=['Brutus', '-Calpurnia']\n",
      "../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "../../shared/corpus\\the_merchant_of_venice.txt\n",
      "../../shared/corpus\\the_rape_of_lucrece.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt\n",
      "../../shared/corpus\\the_tragedy_of_coriolanus.txt\n",
      "../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt\n",
      "../../shared/corpus\\the_tragedy_of_titus_andronicus.txt\n",
      "\n",
      "Processing AND: args=['Caesar', 'Brutus', 'Calpurnia']\n",
      "No documents found\n",
      "\n",
      "Processing AND: args=['Caesar', 'Brutus', '-Calpurnia']\n",
      "../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(execute_query('Caesar'))\n",
    "print_result(execute_query('hello'))\n",
    "print_result(execute_query('Brutus AND Calpurnia'))\n",
    "print_result(execute_query('Brutus OR Hamlet'))\n",
    "print_result(execute_query('Brutus AND NOT Calpurnia'))\n",
    "print_result(execute_query('Caesar AND Brutus AND Calpurnia'))\n",
    "print_result(execute_query('Caesar AND Brutus AND NOT Calpurnia'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-concept",
   "metadata": {},
   "source": [
    "If you wish, you can try to improve your query processor to handle more\n",
    "complex queries, such as the examples below.  This part is optional.\n",
    "\n",
    "Note that the query parser gives `NOT` precedence over `AND`, and `AND`\n",
    "precedence over `OR` in the absence of parentheses in queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "known-balance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AND: args=[OR: args=['Caesar', 'Brutus'], 'Calpurnia']\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "\n",
      "Processing AND: args=[OR: args=['Caesar', 'Brutus'], OR: args=['Calpurnia', 'clown']]\n",
      "../../shared/corpus\\alls_well_that_ends_well.txt\n",
      "../../shared/corpus\\measure_for_measure.txt\n",
      "../../shared/corpus\\the_merchant_of_venice.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "\n",
      "Processing OR: args=[AND: args=['Brutus', 'Calpurnia'], AND: args=['Romeo', 'Juliet']]\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "../../shared/corpus\\the_tragedy_of_romeo_and_juliet.txt\n",
      "\n",
      "Processing OR: args=['Caesar', AND: args=['Brutus', 'Calpurnia']]\n",
      "../../shared/corpus\\alls_well_that_ends_well.txt\n",
      "../../shared/corpus\\cymbeline.txt\n",
      "../../shared/corpus\\king_richard_the_third.txt\n",
      "../../shared/corpus\\measure_for_measure.txt\n",
      "../../shared/corpus\\the_first_part_of_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_life_of_king_henry_the_fifth.txt\n",
      "../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_third_part_of_king_henry_the_sixth.txt\n",
      "../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt\n",
      "../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt\n",
      "../../shared/corpus\\the_tragedy_of_julius_caesar.txt\n",
      "../../shared/corpus\\the_tragedy_of_macbeth.txt\n",
      "../../shared/corpus\\the_tragedy_of_othello_moor_of_venice.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(execute_query('(Caesar OR Brutus) AND Calpurnia'))\n",
    "print_result(execute_query('(Caesar OR Brutus) AND (Calpurnia OR clown)'))\n",
    "print_result(execute_query('(Brutus AND Calpurnia) OR (Romeo AND Juliet)'))\n",
    "print_result(execute_query('Caesar OR Brutus AND Calpurnia'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-friendship",
   "metadata": {},
   "source": [
    "If everything so far was a walk in the park for you, you can even try to\n",
    "implement a method which converts any query into conjunctive normal form\n",
    "(CNF).  Note, however, that converting a boolean formula to CNF is NP hard,\n",
    "and as such we do not require you to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5c1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
