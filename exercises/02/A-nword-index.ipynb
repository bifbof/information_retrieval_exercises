{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a inverted index for n-words\n",
    "=========================================\n",
    "\n",
    "In this week's exercise we will look at enhanced inverted indices.\n",
    "The first form we look at is the biword index (cf. section 2.4.1 of the book)\n",
    "and its generalized form the n-word index.\n",
    "\n",
    "You can think of the biword index as an n-word index with `n = 2`.\n",
    "Similarly a triword index is identical to an n-word index with `n = 3`.\n",
    "Note that an n-word index with `n = 1` should be the same as a standard\n",
    "inverted index with respect to processing boolean queries.\n",
    "\n",
    "In this notebook we provide you with a mostly complete reference\n",
    "implementation for the standard inverted index which you had to implement last\n",
    "week.\n",
    "\n",
    "Your task is to modify `add_document()` to build an n-word index for a\n",
    "configurable n, and to modify `execute_query()` to be able to process phrase\n",
    "queries on the n-word index.\n",
    "A phrase query is a query such as \"Romans countrymen\" which should only return\n",
    "documents that contain this exact phrase.\n",
    "\n",
    "We updated the provided parser to support phrases in arbitrary boolean\n",
    "queries.\n",
    "Any phrase needs to be enclosed in double quotes for the query parser to be\n",
    "able to detect it.\n",
    "The output format for a phrase is a list of the individual terms that make up\n",
    "the phrase, in order.\n",
    "Below we show a few example queries containing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOKUP: args=[['Romans', 'countrymen']]\n",
      "LOOKUP: args=[['Romans', 'countrymen', 'lovers']]\n",
      "LOOKUP: args=[['I', 'think', 'this']]\n"
     ]
    }
   ],
   "source": [
    "from queryparser import parse_query, process_ast\n",
    "print(process_ast(parse_query('\"Romans countrymen\"')))\n",
    "print(process_ast(parse_query('\"Romans countrymen lovers\"')))\n",
    "print(process_ast(parse_query('\"I think this\"')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make phrase queries more interesting we have modified the\n",
    "`tokenize_document()` function from last week to drop all the stop words\n",
    "listed in figure 2.5 in the book.\n",
    "If you're interested, you can acquire the list we use using the python snippet\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with']\n"
     ]
    }
   ],
   "source": [
    "from textutils import stop_words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to remove stop words from your flattened queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(ast):\n",
    "    print(ast)\n",
    "    new_args = []\n",
    "    if ast is not None:\n",
    "        for a in ast.args:\n",
    "            if isinstance(a, list):\n",
    "                new_args.append([x for x in a if not x in stop_words])\n",
    "            elif isinstance(a, Operation):\n",
    "                new_args.append(remove_stop_words(a))\n",
    "            elif a[0] == '-' and a[1:] in stop_words:\n",
    "                pass\n",
    "            elif a in stop_words:\n",
    "                pass\n",
    "            else:\n",
    "                new_args.append(a)\n",
    "        ast.args = new_args\n",
    "    return ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you implement a general n-word index with `n` as a global\n",
    "variable that you can use to change between different indices.\n",
    "\n",
    "While we do not require you to be able to handle arbitrary boolean queries on\n",
    "your n-word index, think about how you would deal with queries that contain\n",
    "non-phrase operands in a more general purpose system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep the imports and global variables in a separate cell, so we can rerun\n",
    "# the code cell without loosing the contents of the index.\n",
    "from queryparser import parse_query, ParseException, process_ast, Operation\n",
    "import glob\n",
    "from textutils import tokenize_document\n",
    "# global variables defining the index\n",
    "documents = dict()\n",
    "the_index = dict()\n",
    "documentid_counter = 1\n",
    "# the path to the corpus\n",
    "corpuspath=\"../../shared/corpus/*.txt\"\n",
    "# the n for n-word\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify `add_document()` to build an n-word index for a configurable `n`. Use strings of `n` words, such as `\"some words\"`, as keys in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document(path):\n",
    "    '''\n",
    "    Add a document to the inverted index. Return the document's document ID.\n",
    "    Remember the mapping from document ID to document in the `documents`\n",
    "    data structure.\n",
    "    '''\n",
    "    # make sure that we access the global variables we have defined\n",
    "    global the_index, documents, documentid_counter, n\n",
    "    # do not re-add the same document.\n",
    "    if path in documents.values():\n",
    "        # find and return document id for document which is already part of index.\n",
    "        for docid, doc in documents.items():\n",
    "            if doc == path:\n",
    "                return docid\n",
    "    docid = documentid_counter\n",
    "    documents[docid] = path\n",
    "    documentid_counter += 1\n",
    "    print(\"Adding '%s' to index\" % path)\n",
    "    for word in tokenize_document(path):\n",
    "        # TODO for assignment: change this inner loop to create a n-word index instead\n",
    "        if word in the_index.keys() and the_index[word][-1] == docid:\n",
    "            continue\n",
    "        the_index.setdefault(word, []).append(docid)\n",
    "    return docid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify `execute_query()` to be able to process phrase queries on the n-word index. Note that all other methods below can remain untouched.\n",
    "As a first cut, don't worry about post-processing to weed out false-positives. You'll optionally be able to revisit your query processing to introduce post-processing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-6-60ea0ea67388>, line 116)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-60ea0ea67388>\"\u001b[0;36m, line \u001b[0;32m116\u001b[0m\n\u001b[0;31m    print(\\\">>> Found phrase '%s' in query '%s', not yet implemented!\\\" % (' '.join(arg), query))\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "def negate(term):\n",
    "    '''\n",
    "    Negate postings list for `term`.  This is not feasible in a real-world\n",
    "    system, but we utilize this for the fallback execution which is fairly\n",
    "    naive.\n",
    "    '''\n",
    "    if term in the_index.keys():\n",
    "        return sorted(set(documents.keys()) - set(the_index[term]))\n",
    "    else:\n",
    "        return list(documents.keys())\n",
    "\n",
    "def execute_query_tree(flat):\n",
    "    '''\n",
    "    Fallback query execution for complex queries, we just recursively evaluate\n",
    "    subtrees.\n",
    "    '''\n",
    "    result = set()\n",
    "    if flat.op == 'AND':\n",
    "        result = set(documents.keys())\n",
    "    for arg in flat.args:\n",
    "        # execute subtree etc\n",
    "        if isinstance(arg, Operation):\n",
    "            temp = execute_query_tree(arg)\n",
    "        elif arg.startswith('-'):\n",
    "            temp = negate(arg[1:])\n",
    "        elif arg not in the_index.keys():\n",
    "            print(\"NOTE: dropping term '%s' because no document contains it\" % arg)\n",
    "        else:\n",
    "            temp = the_index[arg]\n",
    "\n",
    "        if flat.op == 'OR':\n",
    "            result = result | set(temp)\n",
    "        elif flat.op == 'AND':\n",
    "            result = result & set(temp)\n",
    "        elif flat.op == 'NOT':\n",
    "            assert(len(flat.args) == 1)\n",
    "            result = set(documents.keys()) - set(temp)\n",
    "    return sorted(result)\n",
    "\n",
    "def intersect_two(p1, p2):\n",
    "    '''\n",
    "    Intersect two posting lists according to pseudo-code in Introduction\n",
    "    to Information Retrieval, Figure 1.6\n",
    "    '''\n",
    "    answer = []\n",
    "    while p1 != [] and p2 != []:\n",
    "        if p1[0] == p2[0]:\n",
    "            answer.append(p1[0])\n",
    "            p1 = p1[1:]\n",
    "            p2 = p2[1:]\n",
    "        elif p1[0] < p2[0]:\n",
    "            p1 = p1[1:]\n",
    "        else:\n",
    "            p2 = p2[1:]\n",
    "    return answer\n",
    "\n",
    "def intersect(terms):\n",
    "    '''\n",
    "    Intersect posting lists for a list of terms, according to pseudo-code\n",
    "    in Introduction to Information Retrieval, Figure 1.7\n",
    "    '''\n",
    "    postings = [ the_index[t] if not t.startswith('-') else\n",
    "                 negate(t[1:]) for t in terms ]\n",
    "    # calculate word frequencies and sort term,freq pairs in ascending\n",
    "    # order by frequency\n",
    "    freqs = sorted([ (t, len(p)) for t,p in zip(terms, postings) ], key=lambda x: x[1] )\n",
    "    terms, _ = map(list,zip(*freqs))\n",
    "    if terms[0].startswith('-'):\n",
    "        result = negate(terms[0][1:])\n",
    "    else:\n",
    "        result = the_index[terms[0]]\n",
    "    terms = terms[1:]\n",
    "    while terms != [] and result != []:\n",
    "        if terms[0].startswith('-'):\n",
    "            ps = negate(terms[0][1:])\n",
    "        else:\n",
    "            ps = the_index[terms[0]]\n",
    "        result = intersect_two(result, ps)\n",
    "        terms = terms[1:]\n",
    "    return result\n",
    "\n",
    "def execute_query(query):\n",
    "    '''\n",
    "    Execute a boolean query on the inverted index. We only support single\n",
    "    operator queries ATM.  This method returns a list of document ids\n",
    "    which satisfy the query in no particular order (i.e. the order in\n",
    "    which the documents were added most likely :)).\n",
    "    '''\n",
    "    # We use a generated parser to transform the query from a string to an\n",
    "    # AST.\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s'\\n\" % query, e)\n",
    "        return None\n",
    "\n",
    "    # We preprocess the AST to flatten commutative operations, such as\n",
    "    # sequences of ANDs. We also transform 'NOT <term>' arguments into\n",
    "    # '-<term>' to allow smarter processing of AND NOT and OR NOT.\n",
    "    flat = remove_stop_words(process_ast(ast))\n",
    "\n",
    "    # Feel free to remove this print() if you don't find it helpful.\n",
    "    print(\"Flat query repr:\", flat)\n",
    "\n",
    "    args = []\n",
    "    # Perform pre-processing to expand n-word queries\n",
    "    for i, arg in enumerate(flat.args):\n",
    "        if isinstance(arg, list):\n",
    "            # Assume it is a phrase\n",
    "            for w in arg:\n",
    "                assert(isinstance(w, str))\n",
    "\n",
    "            # TODO: implement preprocessing for n-word queries\n",
    "            # how exactly you do this depends on how you've constructed the\n",
    "            # n-word index.\n",
    "            print(\\\">>> Found phrase '%s' in query '%s', not yet implemented!\\\" % (' '.join(arg), query))\n",
    "            assert(False)\n",
    "            # hint:\n",
    "            # if flat.op == 'AND':\n",
    "            # ...\n",
    "        else:\n",
    "            args.append(arg)\n",
    "            \n",
    "    flat.args = args\n",
    "\n",
    "    print(\"Flat query repr after pre-processing:\", flat)\n",
    "    \n",
    "    for arg in flat.args:\n",
    "        if isinstance(arg, Operation):\n",
    "            # as soon as we find a argument to the top-level operation\n",
    "            # which is not just a term, we fall back on the tree query\n",
    "            # execution strategy.\n",
    "            return execute_query_tree(flat)\n",
    "\n",
    "    if flat.op == 'OR':\n",
    "        # For demonstration purposes, utilize python's set() datatype \n",
    "        # to implement OR\n",
    "        results = set()\n",
    "        for arg in flat.args:\n",
    "            if arg.startswith('-'):\n",
    "                print(\"OR NOT not handled (query: '%s'\" % query)\n",
    "                return None\n",
    "            else:\n",
    "                results = results | set(the_index[arg])\n",
    "        return sorted(results)\n",
    "\n",
    "    elif flat.op == 'AND':\n",
    "        return intersect(flat.args)\n",
    "\n",
    "    elif flat.op == 'LOOKUP':\n",
    "        assert(len(flat.args) == 1)\n",
    "        if args[0] not in the_index.keys():\n",
    "            # single term query for term not in vocabulary, return empty list\n",
    "            # of document IDs\n",
    "            return []\n",
    "        else:\n",
    "            # in this case the query was a single term\n",
    "            return the_index[args[0]]\n",
    "    else:\n",
    "        print(\"Cannot handle query '%s', aborting...\" % query)\n",
    "        return None\n",
    "\n",
    "def print_result(docs):\n",
    "    '''\n",
    "    Helper function to convert a list of document IDs back to file names\n",
    "    '''\n",
    "    if not docs:\n",
    "        print(\"No documents found\")\n",
    "        print()\n",
    "        return\n",
    "    # If we got some results, print them\n",
    "    for doc in docs:\n",
    "        print('%d -> %s' % (doc, documents[doc]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows us to build the index, and we run a test boolean query\n",
    "which the provided code should be able to answer satisfactorily.\n",
    "Because `add_document()` does not add documents that are already in the index,\n",
    "this cell can be run multiple times without adverse effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(corpuspath):\n",
    "    add_document(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first 10 items in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(the_index.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a list of example phrase queries that your biword index should be able to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected result: \n",
    "#   the_two_gentlemen_of_verona.txt\n",
    "#   the_two_noble_kinsmen.txt\n",
    "print_result(execute_query('\"THE TWO\"'))\n",
    "\n",
    "# expected result: the_tragedy_of_julius_caesar.txt\n",
    "print_result(execute_query('\"Romans countrymen\"')) \n",
    "\n",
    "# expected result: the_tragedy_of_julius_caesar.txt\n",
    "print_result(execute_query('\"Romans countrymen lovers\"'))\n",
    "\n",
    "# expected result:\n",
    "#   king_henry_the_eighth.txt\n",
    "#   the_tragedy_of_king_lear.txt\n",
    "#   the_second_part_of_king_henry_the_sixth.txt\n",
    "#   the_merchant_of_venice.txt\n",
    "#   the_life_of_timon_of_athens.txt\n",
    "#   the_tragedy_of_hamlet_prince_of_denmark.txt\n",
    "#   alls_well_that_ends_well.txt\n",
    "#   king_richard_the_third.txt\n",
    "#   the_comedy_of_errors.txt\n",
    "#   the_first_part_of_henry_the_sixth.txt\n",
    "#   the_tragedy_of_antony_and_cleopatra.txt\n",
    "#   pericles_prince_of_tyre.txt\n",
    "#   the_first_part_of_king_henry_the_fourth.txt\n",
    "#   the_winters_tale.txt\n",
    "#   the_history_of_troilus_and_cressida.txt\n",
    "#   as_you_like_it.txt\n",
    "#   much_ado_about_nothing.txt\n",
    "#   the_tragedy_of_othello_moor_of_venice.txt\n",
    "print_result(execute_query('\"I think this\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just implement phrase queries on your n-word index as discussed in the book in section 2.4.1, you will notice that the query \"I think this\" returns a number of documents that do not actually contain the phrase \"I think this\" when we query a 2-word index.\n",
    "\n",
    "A way to deal with this is to post-process the results of the query execution engine to drop documents which only contain partial phrases.\n",
    "\n",
    "I decide to implement the postprocessing by calling out to the [`grep`\n",
    "tool](https://en.wikipedia.org/wiki/Grep) tool. What are potential problems with this postprocessing strategy?\n",
    "\n",
    "Optional exercises\n",
    "------------------\n",
    "\n",
    "If you didn't feel challenged by this week's work, you can try your hand at\n",
    "doing (some of) the following implementation work:\n",
    "\n",
    " * implement a postprocessing step which actually correctly filters the\n",
    "   results\n",
    " * Improve your query execution engine to handle phrases in conjunction with\n",
    "   boolean operators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
