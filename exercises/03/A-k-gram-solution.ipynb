{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "social-least",
   "metadata": {},
   "source": [
    "# K-gram indices\n",
    "\n",
    "This week the exercise focuses on k-gram indices, and two uses for them: (i) wildcard queries (`some*e`) and (ii) spell correction (`smoetime` → `sometime`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11c8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tested-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from textutils import tokenize_document\n",
    "from queryparser import parse_query, ast_has_operation, process_ast, Operation, ParseException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-eating",
   "metadata": {},
   "source": [
    "The provided window function returns sliding-n-window subsequences of the input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hollow-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [3, 4], [4, 5], [5, 6]]\n",
      "['ast', 'str', 'tri', 'rin', 'ing']\n"
     ]
    }
   ],
   "source": [
    "def window(seq, n):\n",
    "    for i in range(len(seq) - n + 1):\n",
    "        yield seq[i:i+n]\n",
    "print(list(window([2, 3, 4, 5, 6], 2)))\n",
    "print(list(window(\"astring\", 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominican-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map document titles to document ids\n",
    "documents = {}\n",
    "# A running counter for assigning numerical IDs to documents'''\n",
    "docid_counter = 1\n",
    "# The posting lists\n",
    "the_index = {}\n",
    "# K-gram index\n",
    "kgram_index = dict()\n",
    "# The K in k-gram\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-rebel",
   "metadata": {},
   "source": [
    "Note that from this week onwards the posting lists will be a python `set`: this way we can use its built in deduplication, `&` (intersection), `|` (union), and `-` (difference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confident-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3, 4, 6}\n",
      "{4, 6}\n",
      "{3, 4, 6, 7}\n",
      "{3}\n"
     ]
    }
   ],
   "source": [
    "example_set = {3, 4}\n",
    "example_set.add(6)\n",
    "example_set.add(4)\n",
    "print(example_set)\n",
    "print(example_set & {4, 6, 7})\n",
    "print(example_set | {4, 6, 7})\n",
    "print(example_set - {4, 6, 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-republican",
   "metadata": {},
   "source": [
    "Add documents to the index, and words to the k-gram index. Make sure the posting lists are python `set`s.\n",
    "\n",
    "For the k-gram index you will want to bracket each term in the vocabulary of\n",
    "your standard inverted index with dollar signs, which are used as word\n",
    "boundary markers and are important to process queries where the wildcard is at\n",
    "the beginning or at the end of a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tested-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document ../../shared/corpus\\alls_well_that_ends_well.txt with id 1\n",
      "Added document ../../shared/corpus\\as_you_like_it.txt with id 2\n",
      "Added document ../../shared/corpus\\a_lovers_complaint.txt with id 3\n",
      "Added document ../../shared/corpus\\a_midsummer_nights_dream.txt with id 4\n",
      "Added document ../../shared/corpus\\cymbeline.txt with id 5\n",
      "Added document ../../shared/corpus\\king_henry_the_eighth.txt with id 6\n",
      "Added document ../../shared/corpus\\king_john.txt with id 7\n",
      "Added document ../../shared/corpus\\king_richard_the_second.txt with id 8\n",
      "Added document ../../shared/corpus\\king_richard_the_third.txt with id 9\n",
      "Added document ../../shared/corpus\\loves_labours_lost.txt with id 10\n",
      "Added document ../../shared/corpus\\measure_for_measure.txt with id 11\n",
      "Added document ../../shared/corpus\\much_ado_about_nothing.txt with id 12\n",
      "Added document ../../shared/corpus\\pericles_prince_of_tyre.txt with id 13\n",
      "Added document ../../shared/corpus\\the_comedy_of_errors.txt with id 14\n",
      "Added document ../../shared/corpus\\the_first_part_of_henry_the_sixth.txt with id 15\n",
      "Added document ../../shared/corpus\\the_first_part_of_king_henry_the_fourth.txt with id 16\n",
      "Added document ../../shared/corpus\\the_history_of_troilus_and_cressida.txt with id 17\n",
      "Added document ../../shared/corpus\\the_life_of_king_henry_the_fifth.txt with id 18\n",
      "Added document ../../shared/corpus\\the_life_of_timon_of_athens.txt with id 19\n",
      "Added document ../../shared/corpus\\the_merchant_of_venice.txt with id 20\n",
      "Added document ../../shared/corpus\\the_merry_wives_of_windsor.txt with id 21\n",
      "Added document ../../shared/corpus\\the_passionate_pilgrim.txt with id 22\n",
      "Added document ../../shared/corpus\\the_phoenix_and_the_turtle.txt with id 23\n",
      "Added document ../../shared/corpus\\the_rape_of_lucrece.txt with id 24\n",
      "Added document ../../shared/corpus\\the_second_part_of_king_henry_the_fourth.txt with id 25\n",
      "Added document ../../shared/corpus\\the_second_part_of_king_henry_the_sixth.txt with id 26\n",
      "Added document ../../shared/corpus\\the_sonnets.txt with id 27\n",
      "Added document ../../shared/corpus\\the_taming_of_the_shrew.txt with id 28\n",
      "Added document ../../shared/corpus\\the_tempest.txt with id 29\n",
      "Added document ../../shared/corpus\\the_third_part_of_king_henry_the_sixth.txt with id 30\n",
      "Added document ../../shared/corpus\\the_tragedy_of_antony_and_cleopatra.txt with id 31\n",
      "Added document ../../shared/corpus\\the_tragedy_of_coriolanus.txt with id 32\n",
      "Added document ../../shared/corpus\\the_tragedy_of_hamlet_prince_of_denmark.txt with id 33\n",
      "Added document ../../shared/corpus\\the_tragedy_of_julius_caesar.txt with id 34\n",
      "Added document ../../shared/corpus\\the_tragedy_of_king_lear.txt with id 35\n",
      "Added document ../../shared/corpus\\the_tragedy_of_macbeth.txt with id 36\n",
      "Added document ../../shared/corpus\\the_tragedy_of_othello_moor_of_venice.txt with id 37\n",
      "Added document ../../shared/corpus\\the_tragedy_of_romeo_and_juliet.txt with id 38\n",
      "Added document ../../shared/corpus\\the_tragedy_of_titus_andronicus.txt with id 39\n",
      "Added document ../../shared/corpus\\the_two_gentlemen_of_verona.txt with id 40\n",
      "Added document ../../shared/corpus\\the_two_noble_kinsmen.txt with id 41\n",
      "Added document ../../shared/corpus\\the_winters_tale.txt with id 42\n",
      "Added document ../../shared/corpus\\twelfth_night_or_what_you_will.txt with id 43\n",
      "Added document ../../shared/corpus\\venus_and_adonis.txt with id 44\n"
     ]
    }
   ],
   "source": [
    "# wipe existing data\n",
    "documents = {}\n",
    "docid_counter = 1\n",
    "the_index = {}\n",
    "kgram_index = dict()\n",
    "\n",
    "for doc in glob.glob('../../shared/corpus/*.txt'):\n",
    "    docid = docid_counter\n",
    "    documents[docid] = doc\n",
    "    docid_counter += 1\n",
    "    print(\"Added document %s with id %d\" % (doc, docid))\n",
    "    for word in tokenize_document(doc):\n",
    "        the_index.setdefault(word, set()).add(docid)\n",
    "\n",
    "for word in the_index.keys():\n",
    "    ### <assignment>\n",
    "    for gram in window(\"$\" + word + \"$\", K):\n",
    "        if gram != \"$$\":\n",
    "            kgram_index.setdefault(gram, set()).add(word)\n",
    "    ### </assignment>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-kennedy",
   "metadata": {},
   "source": [
    "Here's a test to check the k-gram index is valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "special-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(kgram_index['$ze'] == {'zeal', 'zeale', 'zealous', 'zeals', 'zeal—', 'zed', 'zenith', 'zephyrs'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-executive",
   "metadata": {},
   "source": [
    "## Wildcard queries\n",
    "\n",
    "Write a function that takes a wildcard query term (`'some*e'`) and returns all the keys to query in the k-gram index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "herbal-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard_parse(q):\n",
    "    ### <assignment>\n",
    "    if q[0] != '*':\n",
    "        q = \"$\" + q\n",
    "    if q[-1] != '*':\n",
    "        q = q + \"$\"\n",
    "    grams = [t for t in window(q, K) if \"*\" not in t]\n",
    "    return grams\n",
    "    ### </assignment>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-license",
   "metadata": {},
   "source": [
    "Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incorporated-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(wildcard_parse('some*e') == ['$so', 'som', 'ome'])\n",
    "assert(wildcard_parse('*where') == ['whe', 'her', 'ere', 're$'])\n",
    "assert(wildcard_parse('some*ere') == ['$so', 'som', 'ome', 'ere', 're$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-domain",
   "metadata": {},
   "source": [
    "Implement querying the k-gram index for matching words, given a certain wildcard query `q`. Here's an high-level description for the implementation:\n",
    "\n",
    "  * parse the wildcard query with `wildcard_parse`;\n",
    "  * check whether any of the returned kgrams are not in the `kgram_index`: if that's the case, there are no matches for this wildcard query, and and empty set should be returned;\n",
    "  * (optionally) compute the number of matches for each kgram and order them from smaller to larger;\n",
    "  * intersect the word matches for each kgram;\n",
    "  * perform post-processing to exclude false positives (we provide some code that converts the wildcard query in a python regex that only matches valid words);\n",
    "  * return the set of matching words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "capable-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kgram_wildcard_query(q):\n",
    "    grams = wildcard_parse(q)\n",
    "    ### <assignment>\n",
    "    for t in grams:\n",
    "        if t not in kgram_index:\n",
    "            print(\"NOTE: no wildcard matches for '%s'\" % q)\n",
    "            return set()\n",
    "    words = [kgram_index[t] for t in grams]\n",
    "    freqs = sorted([ (p, len(p)) for p in words ], key=lambda x: x[1])\n",
    "    result = freqs[0][0]\n",
    "    del freqs[0]\n",
    "    while freqs != []:\n",
    "        result = result & freqs[0][0]\n",
    "        del freqs[0]\n",
    "    post_filter = re.compile(\"^\" + q.replace(\"*\", \"\\\\w*\") + \"$\")\n",
    "    res = {r for r in result if post_filter.match(r) is not None}\n",
    "    print(\"NOTE: wildcard matches for '%s': %s\" % (q, res))\n",
    "    return res\n",
    "    ### </assignment>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-mississippi",
   "metadata": {},
   "source": [
    "Don't forget to perform post-processing to exclude false positives. Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "placed-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: wildcard matches for 'some*e': {'somewhere', 'sometime', 'someone'}\n",
      "NOTE: wildcard matches for '*where': {'elsewhere', 'otherwhere', 'everywhere', 'somewhere', 'nowhere', 'anywhere', 'where'}\n",
      "NOTE: wildcard matches for 'rend*': {'rendezvous', 'rendered', 'render', 'renders', 'renderd', 'rendring', 'rend', 'rendred'}\n",
      "NOTE: wildcard matches for 'man*': {'mannadge', 'managed', 'manyheaded', 'manycolourd', 'managing', 'mandrakes', 'manage', 'manorhouse', 'manlike', 'manatarms', 'mannerly', 'manwhose', 'manner', 'manifoldly', 'mankinde', 'manhoods', 'manual', 'manmy', 'manager', 'manycoloured', 'mane', 'manes', 'manfully', 'manifests', 'manifold', 'manacle', 'mannersas', 'mankind', 'mannd', 'manifest', 'manacles', 'man', 'manofwar', 'manus', 'manywhom', 'manakin', 'mandrake', 'manhood', 'manifested', 'manured', 'mantled', 'mans', 'mangy', 'manchild', 'mangled', 'mannish', 'manors', 'mantles', 'many', 'mansions', 'manure', 'manners', 'mandragora', 'manfew', 'manly', 'mansion', 'manmonster', 'manqueller', 'mannerd', 'mangling', 'manhis', 'manor', 'manna', 'mangle', 'manslaughter', 'mansionry', 'mantle', 'mannersbeing', 'mandate'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert(kgram_wildcard_query('some*e') == {'someone', 'somewhere', 'sometime'})\n",
    "assert(kgram_wildcard_query('*where') == {'otherwhere', 'everywhere', 'nowhere', 'elsewhere', 'anywhere', 'where', 'somewhere'})\n",
    "assert(kgram_wildcard_query('rend*') == {'rend', 'render', 'renderd', 'rendered', 'renders', 'rendezvous', 'rendred', 'rendring'})\n",
    "len(kgram_wildcard_query('man*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-messenger",
   "metadata": {},
   "source": [
    "We provide a generic implementation of `execute_query`. Note that intersection/union operations are implemented with Python's sets. This implementation will invoke `kgram_wildcard_query` when it encounters a wildcard query during preprocessing. It will replace the wildcard term with a disjunction of the matching words (`some*e` → `someone OR somewhere OR sometime`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "floating-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary empty implementation of `spellcorrect`: this will be relevant later\n",
    "def spellcorrect(arg):\n",
    "    return None\n",
    "\n",
    "def execute_query(query):\n",
    "    def negate(postings):\n",
    "        return set(documents.keys()) - postings\n",
    "\n",
    "    try:\n",
    "        ast = parse_query(query)\n",
    "    except ParseException as e:\n",
    "        print(\"Failed to parse query '%s'\\n\" % query, e)\n",
    "        return None\n",
    "\n",
    "    flat = process_ast(ast)\n",
    "\n",
    "    def preprocess_query_tree(tree):\n",
    "        if tree.op == 'LOOKUP':\n",
    "            tree.op = 'AND'\n",
    "        new_args = []\n",
    "        for arg in tree.args:\n",
    "            if isinstance(arg, Operation):\n",
    "                new_args.append(preprocess_query_tree(arg))\n",
    "            elif \"*\" in arg:\n",
    "                kgram_matches = kgram_wildcard_query(arg)\n",
    "                if len(kgram_matches) == 0:\n",
    "                    print(\"NOTE: spell-correcting term '%s' because no document contains it\" % arg)\n",
    "                elif arg.startswith('-'):\n",
    "                    not_op = Operation('NOT', [\n",
    "                        Operation('OR', list(kgram_matches)),])\n",
    "                    new_args.append(not_op)\n",
    "                elif tree.op == 'OR':\n",
    "                    new_args.extend(kgram_matches)\n",
    "                else:\n",
    "                    new_args.append(Operation('OR', kgram_matches))\n",
    "            else:\n",
    "                if not arg.startswith('-') and arg not in the_index:\n",
    "                    print(\"NOTE: spell-correcting term '%s' because no document contains it\" % arg)\n",
    "                    res = spellcorrect(arg)\n",
    "                    # NOTE: spellcorrect is the second part of this exercise, and will return None until implemented\n",
    "                    if res != None and res != []:\n",
    "                        print(\"NOTE: spell-corrections with Jaccard_threshold for '%s': %s\" % (arg, res))\n",
    "                        new_args.append(Operation('OR', list(res)))\n",
    "                else:\n",
    "                    new_args.append(arg)\n",
    "        tree.args = new_args\n",
    "\n",
    "    preprocess_query_tree(flat)\n",
    "\n",
    "    def execute_query_tree(tree):\n",
    "        result = set()\n",
    "        if tree.op == 'AND':\n",
    "            result = set(documents.keys())\n",
    "        for arg in tree.args:\n",
    "            if isinstance(arg, Operation):\n",
    "                temp = execute_query_tree(arg)\n",
    "            elif arg.startswith('-'):\n",
    "                temp = negate(the_index[arg[1:]])\n",
    "            else:\n",
    "                temp = the_index[arg]\n",
    "\n",
    "            if tree.op == 'OR':\n",
    "                result = result | temp\n",
    "            elif tree.op == 'AND':\n",
    "                result = result & temp\n",
    "            elif tree.op == 'NOT':\n",
    "                assert(len(tree.args) == 1)\n",
    "                result = negate(temp)\n",
    "        return result\n",
    "\n",
    "    return execute_query_tree(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "falling-gallery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: wildcard matches for 'some*e': {'somewhere', 'sometime', 'someone'}\n",
      "NOTE: wildcard matches for 'some*e': {'somewhere', 'sometime', 'someone'}\n"
     ]
    }
   ],
   "source": [
    "# adapted to windows lexicographic ordering!\n",
    "assert(execute_query('some*e') == {1, 3, 4, 5, 8, 10, 11, 12, 13, 14, 15, 19, 21, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44})\n",
    "assert(execute_query('some*e AND Romeo') == {38})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-hamilton",
   "metadata": {},
   "source": [
    "## Spell-checking with a k-gram index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "international-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Jaccard_threshold = .25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-biography",
   "metadata": {},
   "source": [
    "Implement spell-correction for a word that was not found in the index, using the k-gram index.\n",
    "\n",
    "  * Compute the k-grams from the input word (remember to include the '\\$' characters);\n",
    "  * find the matching words for each k-gram;\n",
    "  * for each candidate word, check the Jaccard coefficent between the input word and the candidate (remember to include the '\\$' characters);\n",
    "  * if it's above (greater or equal to) the provided threshold, add it to the list of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disturbed-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcorrect(word):\n",
    "    word_grams = set(window('$' + word + '$', 3))\n",
    "    ### <assignment>\n",
    "    matches = [kgram_index[x] for x in word_grams]\n",
    "    word_match_count = len(matches)\n",
    "    all_candidates = set().union(*matches)\n",
    "    outs = []\n",
    "    for candidate in all_candidates:\n",
    "        candidate_grams = set(window('$' + candidate + '$', 3))\n",
    "        candidate_match_count = len(candidate_grams)\n",
    "        intersection_count = len(candidate_grams & word_grams)\n",
    "        union_count = len(candidate_grams.union(word_grams))\n",
    "        if intersection_count / union_count >= Jaccard_threshold:\n",
    "            outs.append((candidate, intersection_count / union_count))\n",
    "    outs = sorted(outs, key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in outs]\n",
    "    ### </assignment>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-stockholm",
   "metadata": {},
   "source": [
    "Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "guided-newport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Beforetime',\n",
       " 'Betimes',\n",
       " 'Sometime',\n",
       " 'bedtime',\n",
       " 'betime',\n",
       " 'betimes',\n",
       " 'lifetime',\n",
       " 'mestime',\n",
       " 'pastime',\n",
       " 'ruttime',\n",
       " 'sometime',\n",
       " 'time',\n",
       " '“Sometime'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(spellcorrect('smoetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "linear-projector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(spellcorrect('smoetime')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-stuff",
   "metadata": {},
   "source": [
    "Note that the assertions that follow assume a Jaccard threshold of .25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "proper-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(set(spellcorrect('smoetime')) == {'betime', 'lifetime', 'sometime', 'pastime', 'bedtime', 'time', 'Betimes', 'mestime', 'ruttime', '“Sometime', 'Sometime', 'betimes', 'Beforetime'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-characterization",
   "metadata": {},
   "source": [
    "You can try your code as part of the query processor, which already calls `spellcorrect` whenever a (non-negated) term cannot be found in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "narrow-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: spell-correcting term 'smoetime' because no document contains it\n",
      "NOTE: spell-corrections with Jaccard_threshold for 'smoetime': ['betime', 'time', 'lifetime', 'Sometime', 'sometime', '“Sometime', 'Beforetime', 'Betimes', 'ruttime', 'mestime', 'bedtime', 'betimes', 'pastime']\n"
     ]
    }
   ],
   "source": [
    "# adapted to windows lexicographic ordering!\n",
    "assert(execute_query('smoetime AND Romeo') == {38})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3755ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
